[
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "The dataset AAPL includes the information about Apple’s stock price for recent half years. In order to make it to be not time sensitive, I add a column called daily return which calculates the difference between open stock price and close stock price in each single day. This dataset also has a column which represents the trading volumn for each day. I am doing clustering for these two variables and to find out the most common situation for Apple’s stock holders.\n\n\n\nK-Means: randomly initialize centroids and assign each data point to the nearest centroid, and then re-locate the centroids, and repeat the process of assigning data points and calculating new means for each centroid, until the centroids do not change significantly after recalculating.\nElbow method: calculate the sum of squared distances of samples to centroids, make a plot and find out the elbow point, where the rate of increase/decrease slow down.\nSilhouette: The silhouette ranges from -1 to 1, higher score means better match for the closest centroid, the highest silhouette score represents the optimal choice.\nDBSCAN: Density-Based Spatial Clustering of Applications with Noise, it creates clusters based on the density of data points. When using this algorithm, we needs to determine the eps (the radius of a neighborhood around a point) and min_samples (the minimum number of points required to form a dense region). We start with a random point, form cluster for all points within the neighborhood, repeat the process until all points get labeled (core, border, noise)\nHierarchical: There are two forms in hierarchical clustering: agglomerative(bottom-up) and divisive(top-down). In agglomerative, we start by treating each point as a cluster, then merge down the closest pair of clusters, repeat the process until there is only one big cluster left. Divisive starts by a large cluster, and then break this cluster down.\n\n\n\n\n\nSelect the trading volumn and daily return.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\nAAPL.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntimestamp\nopen\nhigh\nlow\nclose\nvolume\ndaily_return\n\n\n\n\n0\n0\n2023-12-05\n190.21\n194.40\n190.1800\n193.42\n66628398\nNaN\n\n\n1\n1\n2023-12-04\n189.98\n190.05\n187.4511\n189.43\n43389519\n-2.062868\n\n\n2\n2\n2023-12-01\n190.33\n191.56\n189.2300\n191.24\n45704823\n0.955498\n\n\n3\n3\n2023-11-30\n189.84\n190.32\n188.1900\n189.95\n48794366\n-0.674545\n\n\n4\n4\n2023-11-29\n190.90\n192.09\n188.9700\n189.37\n43014224\n-0.305344\n\n\n\n\n\n\n\n\n\n\n\nAAPL = AAPL.drop('timestamp', axis=1)\nAAPL = AAPL.drop('Unnamed: 0', axis=1)\nAAPL = AAPL.dropna()\nAAPL.head()\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nvolume\ndaily_return\n\n\n\n\n1\n189.98\n190.05\n187.4511\n189.43\n43389519\n-2.062868\n\n\n2\n190.33\n191.56\n189.2300\n191.24\n45704823\n0.955498\n\n\n3\n189.84\n190.32\n188.1900\n189.95\n48794366\n-0.674545\n\n\n4\n190.90\n192.09\n188.9700\n189.37\n43014224\n-0.305344\n\n\n5\n189.78\n191.08\n189.4000\n190.40\n38415419\n0.543909\n\n\n\n\n\n\n\n\n\n\n\n\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(AAPL)\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(df_scaled)\n    sse.append(kmeans.inertia_)\n\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nplt.figure()\nplt.plot(range(1, 11), sse)\nplt.title('Elbow Method for Determining k')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Sum of squared distances (SSE)')\nplt.show()\n\n\n\n\nThe optimal number of clusters is 3\n\n\n\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.neighbors import NearestNeighbors\n\n\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nAAPL['Cluster'] = dbscan.fit_predict(df_scaled)\neps_values = np.arange(0.1, 1.0, 0.1)\nmin_samples_values = range(2, 10)\n\nbest_silhouette_score = -1\nbest_eps = None\nbest_min_samples = None\n\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        cluster_labels = dbscan.fit_predict(df_scaled)\n\n        if len(set(cluster_labels)) &gt; 1 and np.sum(cluster_labels != -1) &lt; len(df_scaled)-1:\n            silhouette_avg = silhouette_score(df_scaled, cluster_labels)\n\n            if silhouette_avg &gt; best_silhouette_score:\n                best_silhouette_score = silhouette_avg\n                best_eps = eps\n                best_min_samples = min_samples\nprint(f\"Best silhouette score is {best_silhouette_score} with eps={best_eps} and min_samples={best_min_samples}\")\n\nBest silhouette score is 0.25298680124699197 with eps=0.9 and min_samples=8\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\n\nnum_clusters = range(2, 10)\nsilhouette_scores = {}\n\nfor n_clusters in num_clusters:\n    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n    cluster_labels = agg_clustering.fit_predict(df_scaled)\n    silhouette_avg = silhouette_score(df_scaled, cluster_labels)\n    silhouette_scores[n_clusters] = silhouette_avg\n\nbest_num_clusters = max(silhouette_scores, key=silhouette_scores.get)\nprint(f'Best number of clusters based on silhouette score: {best_num_clusters}')\n\nbest_agg_clustering = AgglomerativeClustering(n_clusters=best_num_clusters, affinity='euclidean', linkage='ward')\nbest_cluster_labels = best_agg_clustering.fit_predict(df_scaled)\n\nBest number of clusters based on silhouette score: 2\n\n\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n\n\n\n\n\n\nK-Means: k=3\nDBSCAN: min_samples=3, eps=0.8\nAgglomerative: n_clusters=3\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nAAPL_reduced = pca.fit_transform(AAPL)\n\nkmeans = KMeans(n_clusters=3, random_state=5000).fit(AAPL_reduced)\ndbscan = DBSCAN(eps=0.8, min_samples=3).fit(AAPL_reduced)\nagglomerative = AgglomerativeClustering(n_clusters=3, linkage='ward').fit(AAPL_reduced)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].scatter(AAPL_reduced[:, 0], AAPL_reduced[:, 1], c=kmeans.labels_)\naxes[0].set_title('KMeans Clustering')\n\naxes[1].scatter(AAPL_reduced[:, 0], AAPL_reduced[:, 1], c=dbscan.labels_)\naxes[1].set_title('DBSCAN Clustering')\n\naxes[2].scatter(AAPL_reduced[:, 0], AAPL_reduced[:, 1], c=agglomerative.labels_)\naxes[2].set_title('Agglomerative Clustering')\n\nplt.tight_layout()\nplt.show()\n\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\nBased on the plot we got from K-Means, DBSCAN, Agglomerative Clustering, the optimal number of clusters should be 3. Both K-Means and Agglomerative assign the datapoint into 3 clusters, DBSCAN does not constrain the number of clusters, but determines based on eps and minimum sample size."
  },
  {
    "objectID": "Clustering.html#introduction",
    "href": "Clustering.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "The dataset AAPL includes the information about Apple’s stock price for recent half years. In order to make it to be not time sensitive, I add a column called daily return which calculates the difference between open stock price and close stock price in each single day. This dataset also has a column which represents the trading volumn for each day. I am doing clustering for these two variables and to find out the most common situation for Apple’s stock holders."
  },
  {
    "objectID": "Clustering.html#theory",
    "href": "Clustering.html#theory",
    "title": "Clustering",
    "section": "",
    "text": "K-Means: randomly initialize centroids and assign each data point to the nearest centroid, and then re-locate the centroids, and repeat the process of assigning data points and calculating new means for each centroid, until the centroids do not change significantly after recalculating.\nElbow method: calculate the sum of squared distances of samples to centroids, make a plot and find out the elbow point, where the rate of increase/decrease slow down.\nSilhouette: The silhouette ranges from -1 to 1, higher score means better match for the closest centroid, the highest silhouette score represents the optimal choice.\nDBSCAN: Density-Based Spatial Clustering of Applications with Noise, it creates clusters based on the density of data points. When using this algorithm, we needs to determine the eps (the radius of a neighborhood around a point) and min_samples (the minimum number of points required to form a dense region). We start with a random point, form cluster for all points within the neighborhood, repeat the process until all points get labeled (core, border, noise)\nHierarchical: There are two forms in hierarchical clustering: agglomerative(bottom-up) and divisive(top-down). In agglomerative, we start by treating each point as a cluster, then merge down the closest pair of clusters, repeat the process until there is only one big cluster left. Divisive starts by a large cluster, and then break this cluster down."
  },
  {
    "objectID": "Clustering.html#method",
    "href": "Clustering.html#method",
    "title": "Clustering",
    "section": "",
    "text": "Select the trading volumn and daily return.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\nAAPL.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntimestamp\nopen\nhigh\nlow\nclose\nvolume\ndaily_return\n\n\n\n\n0\n0\n2023-12-05\n190.21\n194.40\n190.1800\n193.42\n66628398\nNaN\n\n\n1\n1\n2023-12-04\n189.98\n190.05\n187.4511\n189.43\n43389519\n-2.062868\n\n\n2\n2\n2023-12-01\n190.33\n191.56\n189.2300\n191.24\n45704823\n0.955498\n\n\n3\n3\n2023-11-30\n189.84\n190.32\n188.1900\n189.95\n48794366\n-0.674545\n\n\n4\n4\n2023-11-29\n190.90\n192.09\n188.9700\n189.37\n43014224\n-0.305344\n\n\n\n\n\n\n\n\n\n\n\nAAPL = AAPL.drop('timestamp', axis=1)\nAAPL = AAPL.drop('Unnamed: 0', axis=1)\nAAPL = AAPL.dropna()\nAAPL.head()\n\n\n\n\n\n\n\n\nopen\nhigh\nlow\nclose\nvolume\ndaily_return\n\n\n\n\n1\n189.98\n190.05\n187.4511\n189.43\n43389519\n-2.062868\n\n\n2\n190.33\n191.56\n189.2300\n191.24\n45704823\n0.955498\n\n\n3\n189.84\n190.32\n188.1900\n189.95\n48794366\n-0.674545\n\n\n4\n190.90\n192.09\n188.9700\n189.37\n43014224\n-0.305344\n\n\n5\n189.78\n191.08\n189.4000\n190.40\n38415419\n0.543909\n\n\n\n\n\n\n\n\n\n\n\n\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(AAPL)\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(df_scaled)\n    sse.append(kmeans.inertia_)\n\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nplt.figure()\nplt.plot(range(1, 11), sse)\nplt.title('Elbow Method for Determining k')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Sum of squared distances (SSE)')\nplt.show()\n\n\n\n\nThe optimal number of clusters is 3\n\n\n\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.neighbors import NearestNeighbors\n\n\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nAAPL['Cluster'] = dbscan.fit_predict(df_scaled)\neps_values = np.arange(0.1, 1.0, 0.1)\nmin_samples_values = range(2, 10)\n\nbest_silhouette_score = -1\nbest_eps = None\nbest_min_samples = None\n\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        cluster_labels = dbscan.fit_predict(df_scaled)\n\n        if len(set(cluster_labels)) &gt; 1 and np.sum(cluster_labels != -1) &lt; len(df_scaled)-1:\n            silhouette_avg = silhouette_score(df_scaled, cluster_labels)\n\n            if silhouette_avg &gt; best_silhouette_score:\n                best_silhouette_score = silhouette_avg\n                best_eps = eps\n                best_min_samples = min_samples\nprint(f\"Best silhouette score is {best_silhouette_score} with eps={best_eps} and min_samples={best_min_samples}\")\n\nBest silhouette score is 0.25298680124699197 with eps=0.9 and min_samples=8\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\n\nnum_clusters = range(2, 10)\nsilhouette_scores = {}\n\nfor n_clusters in num_clusters:\n    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n    cluster_labels = agg_clustering.fit_predict(df_scaled)\n    silhouette_avg = silhouette_score(df_scaled, cluster_labels)\n    silhouette_scores[n_clusters] = silhouette_avg\n\nbest_num_clusters = max(silhouette_scores, key=silhouette_scores.get)\nprint(f'Best number of clusters based on silhouette score: {best_num_clusters}')\n\nbest_agg_clustering = AgglomerativeClustering(n_clusters=best_num_clusters, affinity='euclidean', linkage='ward')\nbest_cluster_labels = best_agg_clustering.fit_predict(df_scaled)\n\nBest number of clusters based on silhouette score: 2\n\n\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n\n\n\n\n\n\nK-Means: k=3\nDBSCAN: min_samples=3, eps=0.8\nAgglomerative: n_clusters=3\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nAAPL_reduced = pca.fit_transform(AAPL)\n\nkmeans = KMeans(n_clusters=3, random_state=5000).fit(AAPL_reduced)\ndbscan = DBSCAN(eps=0.8, min_samples=3).fit(AAPL_reduced)\nagglomerative = AgglomerativeClustering(n_clusters=3, linkage='ward').fit(AAPL_reduced)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].scatter(AAPL_reduced[:, 0], AAPL_reduced[:, 1], c=kmeans.labels_)\naxes[0].set_title('KMeans Clustering')\n\naxes[1].scatter(AAPL_reduced[:, 0], AAPL_reduced[:, 1], c=dbscan.labels_)\naxes[1].set_title('DBSCAN Clustering')\n\naxes[2].scatter(AAPL_reduced[:, 0], AAPL_reduced[:, 1], c=agglomerative.labels_)\naxes[2].set_title('Agglomerative Clustering')\n\nplt.tight_layout()\nplt.show()\n\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\nBased on the plot we got from K-Means, DBSCAN, Agglomerative Clustering, the optimal number of clusters should be 3. Both K-Means and Agglomerative assign the datapoint into 3 clusters, DBSCAN does not constrain the number of clusters, but determines based on eps and minimum sample size."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "To learn more about this project visit github https://github.com/anly501/dsan-5000-project-rongmusun. The Repository includes all data files used in this analysis."
  },
  {
    "objectID": "data.html#github-repository",
    "href": "data.html#github-repository",
    "title": "Data",
    "section": "",
    "text": "To learn more about this project visit github https://github.com/anly501/dsan-5000-project-rongmusun. The Repository includes all data files used in this analysis."
  },
  {
    "objectID": "Data Science Topic.html",
    "href": "Data Science Topic.html",
    "title": "Introduction",
    "section": "",
    "text": "This topic is mainly focusing on the stock price and daily return for multiple high-tech companies presents a fascinating area of study, especially considering the dynamic nature of the technology sector in the global market.\nStock refers to the current price at which shares of a high-tech company are trading in the stock market. Discussing stock prices typically involves examining how and why these prices fluctuate over time. Factors influencing stock prices can include the company’s financial performance, investor sentiment, market trends, economic indicators, and news related to the company or its industry. For high-tech companies, specific elements such as technological advancements, product launches, patent acquisitions, regulatory changes, and competition can significantly impact stock prices.\nIn this analysis, we will focus on analyzing the daily return of Apple. Apple’s stock price can be viewed as a barometer of the overall health of the economy. As one of the largest companies in the world, its performance can influence market indices like the Dow Jones Industrial Average and the S&P 500, which are often seen as indicators of economic stability or volatility. Also, Apple is a leading technology company, and its stock price can reflect broader trends in technology and consumer behavior. High demand for Apple products can drive up its stock price, indicating consumer confidence and a strong market for technological innovations. Changes in Apple’s stock price can illustrate investor confidence not only in Apple but in the tech sector as a whole. It can signal how investors perceive the company’s future prospects based on factors like new product launches, corporate governance, and market competition."
  },
  {
    "objectID": "Data Science Topic.html#topic-stock-price-analysis",
    "href": "Data Science Topic.html#topic-stock-price-analysis",
    "title": "Introduction",
    "section": "",
    "text": "This topic is mainly focusing on the stock price and daily return for multiple high-tech companies presents a fascinating area of study, especially considering the dynamic nature of the technology sector in the global market.\nStock refers to the current price at which shares of a high-tech company are trading in the stock market. Discussing stock prices typically involves examining how and why these prices fluctuate over time. Factors influencing stock prices can include the company’s financial performance, investor sentiment, market trends, economic indicators, and news related to the company or its industry. For high-tech companies, specific elements such as technological advancements, product launches, patent acquisitions, regulatory changes, and competition can significantly impact stock prices.\nIn this analysis, we will focus on analyzing the daily return of Apple. Apple’s stock price can be viewed as a barometer of the overall health of the economy. As one of the largest companies in the world, its performance can influence market indices like the Dow Jones Industrial Average and the S&P 500, which are often seen as indicators of economic stability or volatility. Also, Apple is a leading technology company, and its stock price can reflect broader trends in technology and consumer behavior. High demand for Apple products can drive up its stock price, indicating consumer confidence and a strong market for technological innovations. Changes in Apple’s stock price can illustrate investor confidence not only in Apple but in the tech sector as a whole. It can signal how investors perceive the company’s future prospects based on factors like new product launches, corporate governance, and market competition."
  },
  {
    "objectID": "Data Science Topic.html#questions",
    "href": "Data Science Topic.html#questions",
    "title": "Introduction",
    "section": "Questions:",
    "text": "Questions:\n\nWhat is the main trends of change on stock price for high-tech companies in recent half years?\nWhat is the distribution of daily return?\nWhat are the factors that may influence daily return?\nWhat do distribution of daily return indicate?\nHow can we identify group of daily return?\nWhat are the impact of new events on stock price?\nCan we predict stock price accurately by ML methods?\nHow does Apple stock price differ from other companies?\nIs there a significant correlation between market trends and Apple’s stock price?\nHow do financial news relate to stock price?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Introduction",
    "section": "",
    "text": "Hello! My name is Rongmu Sun and I am a first-year graduate student at Georgetown University, currently exploring the new field of Data Science and Analytics (DSAN).\nMy academic journey started at George Mason University, where I completed my undergraduate studies in Accounting major, earning a Bachelor of Science degree in 2023. During my freshman year, I realized that accounting was just not my interest. By a twist of fate, I stumbled upon computer science and felt accomplishment every time I wrote code. So by my second year, I tried to take more courses related to computer science. The real game-changer for me was the course I took in my sophomore year - Management of Information System. It introduced me to databases, business analysis, and that is the first time I heard about a field called data science, and I was really addicted to it. Over the next couple of years, I learned more into this field, taking computer science classes and learning multiple coding languages from C++ and Java to Python and R. I also tried data analysis, I attended a research project from a data science professor, learned the data science process simply, and learned some basic data cleaning methods. The foundation in accounting provided me with the sensitivity of numbers, and those computer science classes I took offer me abilities to approach problems with a computer science perspective, which now serve as a solid base for my DSAN studies. The combination of accounting and computer science has given me a robust foundation in both quantitative analysis and technical computing skills.\nAs a newcomer to the field of Data Science and Analytics (DSAN), I find myself brimming with enthusiasm and curiosity about every aspect of this area. From statistical analysis and machine learning to data visualization and big data technologies, I am actively exploring all these areas to identify where my true passions lie.\nLooking ahead, I am increasingly excited about the prospect of merging my initial exposure to business with my burgeoning tech skills. My aim is to not just interpret data but to translate it into actionable business insights, thereby enabling better decision-making and fostering innovation within organizations. Whether it is through developing data-driven strategies to enhance market competitiveness, optimizing operational efficiencies, or predicting future trends, I see immense potential in applying my dual skill set.\n\n\n\n\nBig fan of SEVENTEEN.\nVideo Games: Apex, Valorant\nOpen to exploring all kinds of outdoor activities with friends\nLove watching animations.\n\n\n\n\nEmail: rs2264@georgetown.edu\nPhone: (571)259-6817\nInstagram: srongmu\nLinkedIn: Rongmu Sun\nCity: Washington, DC"
  },
  {
    "objectID": "about.html#rongmu-sun",
    "href": "about.html#rongmu-sun",
    "title": "Introduction",
    "section": "",
    "text": "Hello! My name is Rongmu Sun and I am a first-year graduate student at Georgetown University, currently exploring the new field of Data Science and Analytics (DSAN).\nMy academic journey started at George Mason University, where I completed my undergraduate studies in Accounting major, earning a Bachelor of Science degree in 2023. During my freshman year, I realized that accounting was just not my interest. By a twist of fate, I stumbled upon computer science and felt accomplishment every time I wrote code. So by my second year, I tried to take more courses related to computer science. The real game-changer for me was the course I took in my sophomore year - Management of Information System. It introduced me to databases, business analysis, and that is the first time I heard about a field called data science, and I was really addicted to it. Over the next couple of years, I learned more into this field, taking computer science classes and learning multiple coding languages from C++ and Java to Python and R. I also tried data analysis, I attended a research project from a data science professor, learned the data science process simply, and learned some basic data cleaning methods. The foundation in accounting provided me with the sensitivity of numbers, and those computer science classes I took offer me abilities to approach problems with a computer science perspective, which now serve as a solid base for my DSAN studies. The combination of accounting and computer science has given me a robust foundation in both quantitative analysis and technical computing skills.\nAs a newcomer to the field of Data Science and Analytics (DSAN), I find myself brimming with enthusiasm and curiosity about every aspect of this area. From statistical analysis and machine learning to data visualization and big data technologies, I am actively exploring all these areas to identify where my true passions lie.\nLooking ahead, I am increasingly excited about the prospect of merging my initial exposure to business with my burgeoning tech skills. My aim is to not just interpret data but to translate it into actionable business insights, thereby enabling better decision-making and fostering innovation within organizations. Whether it is through developing data-driven strategies to enhance market competitiveness, optimizing operational efficiencies, or predicting future trends, I see immense potential in applying my dual skill set."
  },
  {
    "objectID": "about.html#hobbies-and-interests",
    "href": "about.html#hobbies-and-interests",
    "title": "Introduction",
    "section": "",
    "text": "Big fan of SEVENTEEN.\nVideo Games: Apex, Valorant\nOpen to exploring all kinds of outdoor activities with friends\nLove watching animations."
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "Introduction",
    "section": "",
    "text": "Email: rs2264@georgetown.edu\nPhone: (571)259-6817\nInstagram: srongmu\nLinkedIn: Rongmu Sun\nCity: Washington, DC"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stock Price Analysis",
    "section": "",
    "text": "Welcome! This website is the final portfolio project of DSAN 5000.\nThis website includes many components, mainly in three sections.\nThe first section provides the introductory information for me and the background of this analysis, including About me, Data, Code, Introduction and a Data Science Topic page.\nThe second section provides the preparation of data related to this analysis, including Data Gathering, Data Cleaning, and EDAs.\nThe third section is the main part of this analysis, contains multiple analysis on the data, including Naive Bayes, Dimensionality reduction, Clustering, Decision tree.\nThe last section provides the conclusion of this analysis."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "To learn more about this project visit github https://github.com/anly501/dsan-5000-project-rongmusun. The Repository includes all codes used in this analysis."
  },
  {
    "objectID": "code.html#github-repository",
    "href": "code.html#github-repository",
    "title": "Code",
    "section": "",
    "text": "To learn more about this project visit github https://github.com/anly501/dsan-5000-project-rongmusun. The Repository includes all codes used in this analysis."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "In this section, we explore the dataset that we gathered for our analysis, try to visualize the dataset in different ways and provide the insight view of these datasets."
  },
  {
    "objectID": "eda.html#discriptive-statistics",
    "href": "eda.html#discriptive-statistics",
    "title": "Data Exploration",
    "section": "Discriptive Statistics",
    "text": "Discriptive Statistics\nIn this section, we provide the discriptive statistics for our datasets.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\nMSFT = pd.read_csv('MSFT_Cleaned.csv')\nGOOGL = pd.read_csv('GOOGL_Cleaned.csv')\nAMZN = pd.read_csv('AMZN_Cleaned.csv')\nMETA = pd.read_csv('META_Cleaned.csv')\nTSLA = pd.read_csv('TSLA_Cleaned.csv')\n\nAAPL = AAPL.drop('volume', axis=1)\nMSFT = MSFT.drop('volume', axis=1)\nGOOGL = GOOGL.drop('volume', axis=1)\nAMZN = AMZN.drop('volume', axis=1)\nMETA = META.drop('volume', axis=1)\nTSLA = TSLA.drop('volume', axis=1)\n\n\n#Summary statistics for Data\n\nsummary_statistics_AAPL = AAPL.describe()\ncolumns_to_include = [col for col in AAPL.columns if col != 'volume']\nsummary_statistics_AAPL = AAPL[columns_to_include].describe().drop('count')\n\nsummary_statistics_MSFT = MSFT.describe()\ncolumns_to_include = [col for col in MSFT.columns if col != 'volume']\nsummary_statistics_MSFT = MSFT[columns_to_include].describe().drop('count')\n\nsummary_statistics_GOOGL = GOOGL.describe()\ncolumns_to_include = [col for col in GOOGL.columns if col != 'volume']\nsummary_statistics_GOOGL = GOOGL[columns_to_include].describe().drop('count')\n\nsummary_statistics_AMZN = AMZN.describe()\ncolumns_to_include = [col for col in AMZN.columns if col != 'volume']\nsummary_statistics_AMZN = AMZN[columns_to_include].describe().drop('count')\n\nsummary_statistics_META = META.describe()\ncolumns_to_include = [col for col in META.columns if col != 'volume']\nsummary_statistics_META = META[columns_to_include].describe().drop('count')\n\nsummary_statistics_TSLA = TSLA.describe()\ncolumns_to_include = [col for col in TSLA.columns if col != 'volume']\nsummary_statistics_TSLA = TSLA[columns_to_include].describe().drop('count')\n\n\nprint(summary_statistics_AAPL)\nprint(summary_statistics_MSFT)\nprint(summary_statistics_GOOGL)\nprint(summary_statistics_AMZN)\nprint(summary_statistics_META)\nprint(summary_statistics_TSLA)\n\n      Unnamed: 0        open        high        low       close  daily_return\nmean   61.000000  182.621618  184.165332  181.28169  182.798740     -0.034470\nstd    35.651087    7.726758    7.524253    7.81037    7.616109      1.224249\nmin     0.000000  166.910000  168.960000  165.67000  166.890000     -2.258081\n25%    30.500000  176.480000  177.915000  174.96500  176.475000     -0.784270\n50%    61.000000  182.130000  183.890000  180.97000  182.410000     -0.214791\n75%    91.500000  189.840000  190.785000  188.77500  189.740000      0.651839\nmax   122.000000  196.235000  198.230000  195.28000  196.450000      5.044233\n            open        high         low       close  daily_return\nmean  338.371797  341.523667  335.145264  338.381829     -0.084713\nstd    17.439894   17.460940   17.376831   17.758349      1.421001\nmin   310.990000  314.299000  309.450000  312.140000     -3.827645\n25%   327.380000  330.090000  324.450000  327.615000     -1.063090\n50%   333.720000  336.850000  330.390000  333.550000     -0.186183\n75%   345.754000  348.125000  340.890000  345.175000      0.879040\nmax   383.760000  384.300000  378.160000  382.700000      3.910945\n      Unnamed: 0        open        high         low       close  daily_return\nmean   61.000000  129.965325  131.350582  128.727017  130.076179     -0.032348\nstd    35.651087    6.231248    6.189705    6.219766    6.239215      1.751728\nmin     0.000000  116.290000  117.710000  115.350000  116.450000     -5.461437\n25%    30.500000  124.890000  125.784000  123.230000  124.595000     -0.992612\n50%    61.000000  130.710000  132.050000  129.450000  130.860000     -0.102974\n75%    91.500000  135.080000  136.660000  133.980000  135.470000      0.957401\nmax   122.000000  141.050000  141.220000  138.580000  140.550000     10.508717\n      Unnamed: 0        open        high         low       close  daily_return\nmean   61.000000  134.078081  135.608280  132.554260  134.157317     -0.103032\nstd    35.651087    6.588607    6.546159    6.683915    6.777447      1.945700\nmin     0.000000  120.630000  121.639300  118.350000  119.570000     -7.637745\n25%    30.500000  128.880000  130.360000  127.360000  128.670000     -1.304303\n50%    61.000000  133.200000  133.950000  131.619900  133.090000     -0.074515\n75%    91.500000  139.265000  140.660000  137.620000  139.085000      0.878135\nmax   122.000000  147.850000  149.260000  146.880000  147.730000      5.906582\n      Unnamed: 0        open        high         low       close  daily_return\nmean   61.000000  305.800228  309.896241  301.665083  305.852033     -0.114737\nstd    35.651087   16.293083   15.949483   16.412180   16.209030      1.844664\nmin     0.000000  267.170000  271.750000  265.330000  271.050000     -4.230675\n25%    30.500000  295.670000  299.230000  291.255000  294.420000     -1.330240\n50%    61.000000  304.260000  308.659900  300.030000  304.790000     -0.169339\n75%    91.500000  317.295000  320.930000  312.095000  316.285000      1.075186\nmax   122.000000  340.130000  342.920000  338.580000  341.490000      4.459209\n      Unnamed: 0        open        high         low       close  daily_return\nmean   61.000000  248.103603  252.930038  243.238027  248.207642      0.086610\nstd    35.651087   20.364020   20.324808   20.404366   20.519590      3.157984\nmin     0.000000  196.120000  202.800000  194.070000  197.360000     -9.167337\n25%    30.500000  235.370000  239.781650  231.650000  235.840000     -1.610762\n50%    61.000000  250.000000  255.399900  244.590000  251.120000     -0.005460\n75%    91.500000  261.235000  267.550000  257.429550  261.465000      1.807493\nmax   122.000000  296.040000  299.290000  289.520100  293.340000     10.787372"
  },
  {
    "objectID": "eda.html#visualizations",
    "href": "eda.html#visualizations",
    "title": "Data Exploration",
    "section": "Visualizations",
    "text": "Visualizations\nFirst, we show the price change directly in a line plot.\n\n#Line plot for closing price for each company\nAAPL['timestamp'] = pd.to_datetime(AAPL['timestamp'])\nAAPL.set_index('timestamp', inplace=True)\nMSFT['timestamp'] = pd.to_datetime(MSFT['timestamp'])\nMSFT.set_index('timestamp', inplace=True)\nGOOGL['timestamp'] = pd.to_datetime(GOOGL['timestamp'])\nGOOGL.set_index('timestamp', inplace=True)\nAMZN['timestamp'] = pd.to_datetime(AMZN['timestamp'])\nAMZN.set_index('timestamp', inplace=True)\nMETA['timestamp'] = pd.to_datetime(META['timestamp'])\nMETA.set_index('timestamp', inplace=True)\nTSLA['timestamp'] = pd.to_datetime(TSLA['timestamp'])\nTSLA.set_index('timestamp', inplace=True)\n\nclosing_prices = pd.DataFrame({\n    'AAPL': AAPL['close'],\n    'MSFT': MSFT['close'],\n    'GOOGL': GOOGL['close'],\n    'AMZN': AMZN['close'],\n    'META': META['close'],\n    'TSLA': TSLA['close']\n})\nplt.figure(figsize=(15, 8))\nfor ticker in closing_prices.columns:\n    plt.plot(closing_prices.index, closing_prices[ticker], label=ticker)\n\nplt.title('Closing Prices Over Time')\nplt.xlabel('Date')\nplt.ylabel('Price (USD)')\nplt.legend()\nplt.show()\n\n\n\n\nThe line plot shows the overall stock price. Each company’s stock price trend is distinct. AAPL (Apple) shows a generally upward trend, suggesting a period of growth or positive performance. TSLA (Tesla), on the other hand, exhibits a more volatile trend with significant drops, especially towards the end of the period.\nTSLA’s stock price shows notable volatility compared to the others, with sharp increases and decreases. This might be indicative of market reactions to news or events specifically related to Tesla.\nThere seems to be a period where most stocks show a downward trend (particularly noticeable with TSLA, AMZN, and GOOGL), which could point to a market-wide event or sentiment that negatively impacted these stocks.\nA noticeable trend across multiple stocks is a general decrease at specific points in time (2023-07 to 2023-08). This could be indicative of market-wide events or sentiments that are negatively impacting the tech sector as a whole.\nThen we want to show the distribution for daily return for each company.\n\n# Distribution for daily return\nimport seaborn as sns\nfor column in AAPL.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(AAPL[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for AAPL')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in MSFT.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(MSFT[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for MSFT')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in GOOGL.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(GOOGL[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for GOOGL')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in AMZN.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(AMZN[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for AMZN')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in META.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(META[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for META')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in TSLA.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(TSLA[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for TSLA')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn all histograms, the distribution of daily returns does not perfectly align with the normal distribution curve, which is common in real-world financial data due to market inefficiencies, company-specific news, and macroeconomic events affecting stock prices. The varying shapes and spreads of these distributions can provide insights into the risk and volatility associated with each company’s stock. Investors often analyze these distributions to make decisions about portfolio risk management and investment strategies.\nThe center of the distribution indicates the major point of daily return, if it is right skewed, then the company has more days with positive daily return, such as Amazon, some are close to 0, means a balance between positive days and negative days. Some distribution have multiple peeks, indicating a high volatility, such as Tesla.\nWe also create a box plot for daily return.\n\ndaily_returns = closing_prices.pct_change().dropna()\n\nsns.boxplot(data=daily_returns)\n\nplt.title('Box Plot of Daily Returns for Different Stocks')\nplt.ylabel('Daily Return')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nThe medians (central lines within the boxes) are close to zero for most stocks, indicating that the middle value of returns is relatively low or around a neutral change in stock price. The interquartile ranges (IQRs, represented by the length of the boxes) vary, with TSLA (Tesla) showing the widest IQR, suggesting more variability or volatility in daily returns compared to the others.\nThe circles represent outliers, which are daily returns that lie beyond the expected range. Notably, TSLA has several outliers, which indicates days with exceptional gains or losses compared to typical daily movements.\nThe box plots for AAPL, MSFT, and GOOGL appear relatively symmetrical around their medians, suggesting an even distribution of daily returns above and below the median. AMZN and META show a slight shift towards the negative end, indicating that daily returns are more frequently on the lower side.\nWe are trying to figure out if the daily return for different companies have any correlations, so we create a heatmap for each companies.\n\ncorrelation_matrix = daily_returns.corr()\n\nsns.heatmap(correlation_matrix, annot=True, cmap='Blues', linewidths=.5)\nplt.title('Correlation Matrix for Daily Returns of Stocks')\nplt.show()\n\n\n\n\nA correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. The value is in the range of -1 to 1.\nIn this plot, GOOGL and AMZN show a relatively high positive correlation of 0.59, suggesting that their stock returns tend to move in the same direction. Similarly, GOOGL and META have a correlation of 0.61, AMZN and META also have a correlation of 0.61."
  },
  {
    "objectID": "Decision Tree.html",
    "href": "Decision Tree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "A Decision Tree (DT) is a method used in data analysis that resembles a tree structure in its operation. It’s a series of sequential decisions, each branching out based on the characteristics of the data. In each step, the decision tree looks at one aspect of the data and decides which way to go next, just like following a path based on signposts.\nThe primary goal of a Decision Tree is to split the data into distinct groups based on certain features. For instance, when analyzing stock prices, a decision tree might split the data based on whether the stock’s price was above or below a certain threshold on a given day. These splits are chosen to best separate the data into groups with different behaviors or characteristics.\nHowever, Decision Trees can sometimes oversimplify or overfit the data. This means they might not perform well when faced with new, unseen data because they’ve become too tailored to the specific data they were trained on.\nTo address this, we often use the Random Forest (RF) approach. This method combines the predictions of multiple Decision Trees to improve accuracy and reliability. Each tree in a Random Forest is built from a randomly selected subset of the data, making each tree a bit different. When it’s time to make a prediction, the Random Forest takes an average or a majority vote of all these trees. This ensemble approach helps in balancing out biases and errors of individual trees, leading to better performance on new, unseen data.\nFrom a big-picture perspective, both Decision Trees and Random Forests are methods used to analyze complex datasets and make predictions. They are particularly useful in situations where the relationships in the data are not immediately obvious. In the context of stock prices, these methods help in identifying patterns and trends that might indicate future price movements, offering valuable insights for decision-making processes.\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport pandas as pd\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\nAAPL.head()\n\ndf = AAPL.drop(columns=['Unnamed: 0', 'timestamp', 'open', 'close'])\ndf.dropna()\n\n\n\n\n\n\n\n\nhigh\nlow\nvolume\ndaily_return\n\n\n\n\n1\n190.05\n187.4511\n43389519\n-2.062868\n\n\n2\n191.56\n189.2300\n45704823\n0.955498\n\n\n3\n190.32\n188.1900\n48794366\n-0.674545\n\n\n4\n192.09\n188.9700\n43014224\n-0.305344\n\n\n5\n191.08\n189.4000\n38415419\n0.543909\n\n\n...\n...\n...\n...\n...\n\n\n118\n186.99\n184.2700\n101256225\n-0.048646\n\n\n119\n186.52\n183.7800\n65433166\n0.589444\n\n\n120\n184.39\n182.0200\n57462882\n-1.107467\n\n\n121\n184.15\n182.4400\n54929129\n-0.347921\n\n\n122\n183.89\n180.9700\n54754995\n0.261852\n\n\n\n\n122 rows × 4 columns\n\n\n\n\nX = df[['high', 'low']]\ny = df['daily_return']\ndf['daily_return'].fillna(df['daily_return'].mean(), inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeRegressor(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n#mse = mean_squared_error(y_test, y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\n\nrmse, model\n\n\n(1.3036617804945074, DecisionTreeRegressor(random_state=42))\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.dummy import DummyClassifier\n\n\ndf['return_class'] = (df['daily_return'] &gt; 0).astype(int)\nclass_counts = df['return_class'].value_counts()\nprint(class_counts)\n\nX = df[['high', 'low', 'volume']]\ny = df['return_class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = DecisionTreeRegressor(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\ndummy_uniform = DummyClassifier(strategy=\"uniform\", random_state=42)\ndummy_uniform.fit(X_train, y_train)\ny_pred_uniform = dummy_uniform.predict(X_test)\n\naccuracy_uniform = accuracy_score(y_test, y_pred_uniform)\nprecision_uniform = precision_score(y_test, y_pred_uniform, zero_division=0)\nrecall_uniform = recall_score(y_test, y_pred_uniform, zero_division=0)\n\ndummy_stratified = DummyClassifier(strategy=\"stratified\", random_state=42)\ndummy_stratified.fit(X_train, y_train)\ny_pred_stratified = dummy_stratified.predict(X_test)\n\naccuracy_stratified = accuracy_score(y_test, y_pred_stratified)\nprecision_stratified = precision_score(y_test, y_pred_stratified, zero_division=0)\nrecall_stratified = recall_score(y_test, y_pred_stratified, zero_division=0)\n\n(accuracy_uniform, precision_uniform, recall_uniform), (accuracy_stratified, precision_stratified, recall_stratified)\n\nreturn_class\n0    69\n1    54\nName: count, dtype: int64\n\n\n((0.56, 0.36363636363636365, 0.5), (0.56, 0.3333333333333333, 0.375))\n\n\nThe classification typically classify the dataset into 2 groups, positive daily return and negative daily return. The dataset is raletively balanced as it has almost half and half for both 0 and 1. The uniform strategy classifier performs better than the stratified classifier, with a higher accuracy of 56%.\n\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\nfeature_names_list = X.columns.tolist()\nplt.figure(figsize=(12,8))\nplot_tree(model, filled=True, feature_names=feature_names_list)\nplt.show()\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': range(1, 10),\n    'min_samples_split': range(2, 10),\n    'min_samples_leaf': range(1, 10)\n}\ndtree = DecisionTreeRegressor(random_state=42)\ngrid_search = GridSearchCV(dtree, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best parameters:\", best_params)\nprint(\"Best score:\", best_score)\n\noptimal_model = DecisionTreeRegressor(**best_params, random_state=42)\noptimal_model.fit(X_train, y_train)\n\nplt.figure(figsize=(12,8))\nplot_tree(optimal_model, filled=True, feature_names=feature_names_list)\nplt.show()\n\nBest parameters: {'max_depth': 1, 'min_samples_leaf': 8, 'min_samples_split': 2}\nBest score: -0.25854706191243626\n\n\n\n\n\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10,7))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n\n\n\n\n\n\nThe optimal decision tree has too few leaves, it might due to that the dataset does not fit the decision tree algorithm very well. It could be imporved by setting other parameters but those might not be the optimal choice.\nThe plot of confusion matrix shows that the there’s a high number of correct prediction (13 on true positive, 3 on false negative) and small number of incorrect prediction (5 on false positive and 4 on true negative), indicating that the model overall perform well in predicting."
  },
  {
    "objectID": "Decision Tree.html#methods",
    "href": "Decision Tree.html#methods",
    "title": "Decision Tree",
    "section": "",
    "text": "A Decision Tree (DT) is a method used in data analysis that resembles a tree structure in its operation. It’s a series of sequential decisions, each branching out based on the characteristics of the data. In each step, the decision tree looks at one aspect of the data and decides which way to go next, just like following a path based on signposts.\nThe primary goal of a Decision Tree is to split the data into distinct groups based on certain features. For instance, when analyzing stock prices, a decision tree might split the data based on whether the stock’s price was above or below a certain threshold on a given day. These splits are chosen to best separate the data into groups with different behaviors or characteristics.\nHowever, Decision Trees can sometimes oversimplify or overfit the data. This means they might not perform well when faced with new, unseen data because they’ve become too tailored to the specific data they were trained on.\nTo address this, we often use the Random Forest (RF) approach. This method combines the predictions of multiple Decision Trees to improve accuracy and reliability. Each tree in a Random Forest is built from a randomly selected subset of the data, making each tree a bit different. When it’s time to make a prediction, the Random Forest takes an average or a majority vote of all these trees. This ensemble approach helps in balancing out biases and errors of individual trees, leading to better performance on new, unseen data.\nFrom a big-picture perspective, both Decision Trees and Random Forests are methods used to analyze complex datasets and make predictions. They are particularly useful in situations where the relationships in the data are not immediately obvious. In the context of stock prices, these methods help in identifying patterns and trends that might indicate future price movements, offering valuable insights for decision-making processes.\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport pandas as pd\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\nAAPL.head()\n\ndf = AAPL.drop(columns=['Unnamed: 0', 'timestamp', 'open', 'close'])\ndf.dropna()\n\n\n\n\n\n\n\n\nhigh\nlow\nvolume\ndaily_return\n\n\n\n\n1\n190.05\n187.4511\n43389519\n-2.062868\n\n\n2\n191.56\n189.2300\n45704823\n0.955498\n\n\n3\n190.32\n188.1900\n48794366\n-0.674545\n\n\n4\n192.09\n188.9700\n43014224\n-0.305344\n\n\n5\n191.08\n189.4000\n38415419\n0.543909\n\n\n...\n...\n...\n...\n...\n\n\n118\n186.99\n184.2700\n101256225\n-0.048646\n\n\n119\n186.52\n183.7800\n65433166\n0.589444\n\n\n120\n184.39\n182.0200\n57462882\n-1.107467\n\n\n121\n184.15\n182.4400\n54929129\n-0.347921\n\n\n122\n183.89\n180.9700\n54754995\n0.261852\n\n\n\n\n122 rows × 4 columns\n\n\n\n\nX = df[['high', 'low']]\ny = df['daily_return']\ndf['daily_return'].fillna(df['daily_return'].mean(), inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeRegressor(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n#mse = mean_squared_error(y_test, y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\n\nrmse, model\n\n\n(1.3036617804945074, DecisionTreeRegressor(random_state=42))"
  },
  {
    "objectID": "Decision Tree.html#class-distribution-and-baseline-model-for-comparison",
    "href": "Decision Tree.html#class-distribution-and-baseline-model-for-comparison",
    "title": "Decision Tree",
    "section": "",
    "text": "from sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.dummy import DummyClassifier\n\n\ndf['return_class'] = (df['daily_return'] &gt; 0).astype(int)\nclass_counts = df['return_class'].value_counts()\nprint(class_counts)\n\nX = df[['high', 'low', 'volume']]\ny = df['return_class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = DecisionTreeRegressor(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\ndummy_uniform = DummyClassifier(strategy=\"uniform\", random_state=42)\ndummy_uniform.fit(X_train, y_train)\ny_pred_uniform = dummy_uniform.predict(X_test)\n\naccuracy_uniform = accuracy_score(y_test, y_pred_uniform)\nprecision_uniform = precision_score(y_test, y_pred_uniform, zero_division=0)\nrecall_uniform = recall_score(y_test, y_pred_uniform, zero_division=0)\n\ndummy_stratified = DummyClassifier(strategy=\"stratified\", random_state=42)\ndummy_stratified.fit(X_train, y_train)\ny_pred_stratified = dummy_stratified.predict(X_test)\n\naccuracy_stratified = accuracy_score(y_test, y_pred_stratified)\nprecision_stratified = precision_score(y_test, y_pred_stratified, zero_division=0)\nrecall_stratified = recall_score(y_test, y_pred_stratified, zero_division=0)\n\n(accuracy_uniform, precision_uniform, recall_uniform), (accuracy_stratified, precision_stratified, recall_stratified)\n\nreturn_class\n0    69\n1    54\nName: count, dtype: int64\n\n\n((0.56, 0.36363636363636365, 0.5), (0.56, 0.3333333333333333, 0.375))\n\n\nThe classification typically classify the dataset into 2 groups, positive daily return and negative daily return. The dataset is raletively balanced as it has almost half and half for both 0 and 1. The uniform strategy classifier performs better than the stratified classifier, with a higher accuracy of 56%.\n\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\nfeature_names_list = X.columns.tolist()\nplt.figure(figsize=(12,8))\nplot_tree(model, filled=True, feature_names=feature_names_list)\nplt.show()"
  },
  {
    "objectID": "Decision Tree.html#model-tuning",
    "href": "Decision Tree.html#model-tuning",
    "title": "Decision Tree",
    "section": "",
    "text": "from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': range(1, 10),\n    'min_samples_split': range(2, 10),\n    'min_samples_leaf': range(1, 10)\n}\ndtree = DecisionTreeRegressor(random_state=42)\ngrid_search = GridSearchCV(dtree, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best parameters:\", best_params)\nprint(\"Best score:\", best_score)\n\noptimal_model = DecisionTreeRegressor(**best_params, random_state=42)\noptimal_model.fit(X_train, y_train)\n\nplt.figure(figsize=(12,8))\nplot_tree(optimal_model, filled=True, feature_names=feature_names_list)\nplt.show()\n\nBest parameters: {'max_depth': 1, 'min_samples_leaf': 8, 'min_samples_split': 2}\nBest score: -0.25854706191243626\n\n\n\n\n\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10,7))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()"
  },
  {
    "objectID": "Decision Tree.html#final-results",
    "href": "Decision Tree.html#final-results",
    "title": "Decision Tree",
    "section": "",
    "text": "The optimal decision tree has too few leaves, it might due to that the dataset does not fit the decision tree algorithm very well. It could be imporved by setting other parameters but those might not be the optimal choice.\nThe plot of confusion matrix shows that the there’s a high number of correct prediction (13 on true positive, 3 on false negative) and small number of incorrect prediction (5 on false positive and 4 on true negative), indicating that the model overall perform well in predicting."
  },
  {
    "objectID": "Dimensionality reduction.html",
    "href": "Dimensionality reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "PCA stands for Principal Component Analysis. It is a statistical technique used in the field of machine learning and data analysis to emphasize variation and bring out strong patterns in a dataset. PCA is commonly used as a method of dimensionality reduction, especially in contexts where there are many variables or features, but not all of them are equally useful for analysis.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\n\nAAPL = AAPL.drop('timestamp', axis=1)\nAAPL = AAPL.drop('Unnamed: 0', axis=1)\nAAPL = AAPL.dropna()\n\nscaler = StandardScaler()\nscaled_df = scaler.fit_transform(AAPL)\n\npca = PCA()\npca.fit(scaled_df)\ncumulative_variance = np.cumsum(pca.explained_variance_ratio_)\noptimal_num_components = np.where(cumulative_variance &gt;= 0.95)[0][0] + 1\n\nprint(optimal_num_components)\n\nplt.figure(figsize=(8, 5))\nplt.plot(cumulative_variance, marker='o')\nplt.axhline(y=0.95, color='r', linestyle='--')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Explained Variance by Number of Principal Components')\nplt.show()\n\n3\n\n\n\n\n\nThe optimal components would be 2 as the first one explains 85%, the second one explains 15%, and adding more does not increase it significantly.\n\n\n\n\nfrom sklearn.manifold import TSNE\n\n\nperplexities = [5, 30, 50, 75]\nfig, axes = plt.subplots(1, len(perplexities), figsize=(15, 4))\nfor i, perplexity in enumerate(perplexities):\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=1212)\n    tsne_result = tsne.fit_transform(scaled_df)\n    axes[i].scatter(tsne_result[:, 0], tsne_result[:, 1])\n    axes[i].set_title(f'Perplexity: {perplexity}')\nplt.tight_layout()\nplt.show()\n\n\n\n\nWith the increase of perplexity, the points become more dense and the clusers are more clear. However, when the perplexity increase more than 50, the clusters are merging into 1, which cannot represent the structure of points.\n\n\n\nThe PCA plot is effective as it clearly shows the elbow in the plot, and gives a clear result that the optimal number of components is 2. The t-SNE is also effective as we can see a significant increase in the clustering when the perplexity goes up, and we can also see the merging in clusters when the perplexity goes too high, so it also gives a kindly proper range of perplexity which is around 50.\nThe dimensionality reduction is firstly done by using PCA, and the plot shows that the optimal number of components is 2. Then I use 2 as the number of components, apply t-SNE with different perplexity, and find out a clearer cluster plot with perplexity = 50."
  },
  {
    "objectID": "Dimensionality reduction.html#pca",
    "href": "Dimensionality reduction.html#pca",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "PCA stands for Principal Component Analysis. It is a statistical technique used in the field of machine learning and data analysis to emphasize variation and bring out strong patterns in a dataset. PCA is commonly used as a method of dimensionality reduction, especially in contexts where there are many variables or features, but not all of them are equally useful for analysis.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\n\nAAPL = AAPL.drop('timestamp', axis=1)\nAAPL = AAPL.drop('Unnamed: 0', axis=1)\nAAPL = AAPL.dropna()\n\nscaler = StandardScaler()\nscaled_df = scaler.fit_transform(AAPL)\n\npca = PCA()\npca.fit(scaled_df)\ncumulative_variance = np.cumsum(pca.explained_variance_ratio_)\noptimal_num_components = np.where(cumulative_variance &gt;= 0.95)[0][0] + 1\n\nprint(optimal_num_components)\n\nplt.figure(figsize=(8, 5))\nplt.plot(cumulative_variance, marker='o')\nplt.axhline(y=0.95, color='r', linestyle='--')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Explained Variance by Number of Principal Components')\nplt.show()\n\n3\n\n\n\n\n\nThe optimal components would be 2 as the first one explains 85%, the second one explains 15%, and adding more does not increase it significantly."
  },
  {
    "objectID": "Dimensionality reduction.html#t-sne",
    "href": "Dimensionality reduction.html#t-sne",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "from sklearn.manifold import TSNE\n\n\nperplexities = [5, 30, 50, 75]\nfig, axes = plt.subplots(1, len(perplexities), figsize=(15, 4))\nfor i, perplexity in enumerate(perplexities):\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=1212)\n    tsne_result = tsne.fit_transform(scaled_df)\n    axes[i].scatter(tsne_result[:, 0], tsne_result[:, 1])\n    axes[i].set_title(f'Perplexity: {perplexity}')\nplt.tight_layout()\nplt.show()\n\n\n\n\nWith the increase of perplexity, the points become more dense and the clusers are more clear. However, when the perplexity increase more than 50, the clusters are merging into 1, which cannot represent the structure of points."
  },
  {
    "objectID": "Dimensionality reduction.html#evaluation-and-comparison",
    "href": "Dimensionality reduction.html#evaluation-and-comparison",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "The PCA plot is effective as it clearly shows the elbow in the plot, and gives a clear result that the optimal number of components is 2. The t-SNE is also effective as we can see a significant increase in the clustering when the perplexity goes up, and we can also see the merging in clusters when the perplexity goes too high, so it also gives a kindly proper range of perplexity which is around 50.\nThe dimensionality reduction is firstly done by using PCA, and the plot shows that the optimal number of components is 2. Then I use 2 as the number of components, apply t-SNE with different perplexity, and find out a clearer cluster plot with perplexity = 50."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Our Naive Bayes classifier seems to struggle significantly with this particular task, especially in correctly predicting the positive class for AAPL stock price movement. The large number of false negatives implies that the model often predicts that the stock price will not increase when, in fact, it does. This could be particularly problematic if the model is used for investment decisions, as it would suggest missing out on potential gains. The low recall for the positive class suggests that the features used to predict the stock price increase are not capturing the patterns effectively, or that the Naive Bayes assumption of feature independence does not hold well for this financial data. Overall, while the model may be moderately accurate in identifying days when the stock price won’t increase, it is not reliable for predicting increases in stock price. For any practical application, such as trading, this model would likely require significant improvement or a different modeling approach to be useful."
  },
  {
    "objectID": "conclusion.html#naive-bayes",
    "href": "conclusion.html#naive-bayes",
    "title": "Conclusion",
    "section": "",
    "text": "Our Naive Bayes classifier seems to struggle significantly with this particular task, especially in correctly predicting the positive class for AAPL stock price movement. The large number of false negatives implies that the model often predicts that the stock price will not increase when, in fact, it does. This could be particularly problematic if the model is used for investment decisions, as it would suggest missing out on potential gains. The low recall for the positive class suggests that the features used to predict the stock price increase are not capturing the patterns effectively, or that the Naive Bayes assumption of feature independence does not hold well for this financial data. Overall, while the model may be moderately accurate in identifying days when the stock price won’t increase, it is not reliable for predicting increases in stock price. For any practical application, such as trading, this model would likely require significant improvement or a different modeling approach to be useful."
  },
  {
    "objectID": "conclusion.html#dimensionality-reduction",
    "href": "conclusion.html#dimensionality-reduction",
    "title": "Conclusion",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nWe create t-SNE (t-Distributed Stochastic Neighbor Embedding) visualizations for different values of perplexity (5, 30, 50, 75), applied to a dataset concerning Apple (AAPL) stock prices. At the perplexity of 50, the points are clearly forming distinct clusters, with gaps between them. This indicates that the global structure is emphasized, and the algorithm is finding more meaningful clusters within the data. It suggests that there are clusters within the AAPL stock price dataset that can be more or less emphasized by adjusting the perplexity parameter."
  },
  {
    "objectID": "conclusion.html#clustering",
    "href": "conclusion.html#clustering",
    "title": "Conclusion",
    "section": "Clustering",
    "text": "Clustering\nThe optimal number of clusters for KMeans is 3, and the cluster we create from KMeans and Agglomerative is similar. DBSCAN’s silhouette score suggests that the clustering structure is weak, with a score of approximately 0.25, which is relatively low, indicating DBSCAN might not be the most appropriate method for this dataset. The clusters formed by the Agglomerative Clustering with Ward linkage seem to be relatively well-defined, producing more defined and possibly more meaningful clusters for Apple’s stock price."
  },
  {
    "objectID": "conclusion.html#decision-tree",
    "href": "conclusion.html#decision-tree",
    "title": "Conclusion",
    "section": "Decision Tree",
    "text": "Decision Tree\nIn our analysis of Apple’s stock prices, the decision tree model we used to predict daily returns turned out to be quite basic, with only two outcomes. This indicates that the model may not fully capture the complexities and variables that drive the stock market’s fluctuations. The model’s simplicity could suggest that it’s not adequately tailored to the nuances of stock price movements. Stock prices can be influenced by a number of factors, ranging from company performance and investor sentiment to broader economic indicators. A model with only two branches may not account for these diverse influences adequately.\nHowever, even with its limited structure, the model managed to correctly predict the direction of stock price movement with a degree of accuracy. While it certainly didn’t catch every nuance, the high number of correct predictions indicates that the model has captured some underlying trends in the data. It seems to have identified a basic pattern in the stock’s movements that, more often than not, pointed in the right direction.\nThe key takeaway here is that while the decision tree provided some correct predictions, its oversimplified structure suggests there is much room for improvement. By incorporating more data or trying different modeling techniques, we might develop a more sophisticated model that can better navigate the intricacies of stock market predictions. With further refinement, we aim to enhance the accuracy and reliability of our predictions, ultimately striving for a model that closely aligns with the dynamic nature of the stock market."
  },
  {
    "objectID": "Naïve Bayes.html",
    "href": "Naïve Bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a probabilistic machine learning algorithm based on the Bayes’ Theorem, used for classification tasks. it assumes that features in a dataset are mutually independent, which is a strong assumption.\nFor each class and feature, the method calculates the mean and variance, which will be used to make probability estimates based on the Gaussian distribution. For each instance to be classified, we calculate the posterior probability for every class using the Bayes formula. Assume it is independence between features. So, for multiple features, the posterior probability is the product of individual probabilities of each feature. Then we compare the posterior probabilities across classes and assign the class with the highest probability to the instance, and the class with highest probability is our prediction\nThere are three types of Naive Bayes Classifier: Gaussian, Multinomial, Bernoulli. They share common principles but differ in handling the type of data they are applied to.\nGaussian Naive Bayes: Gaussian is best used in cases where features are continuous and can be assumed to have a Gaussian distribution. It assumes that features follow a normal distribution. Instead of using discrete counts, it uses the mean and variance of the features to estimate the probabilities. If the features are continuous, it assumes that these features are sampled from a Gaussian distribution (bell curve).\nMultinomial Naive Bayes: Multinomial is primarily used for document classification problems where the features are related to word counts or frequencies within the documents. This model is based on frequency counts. It calculates the likelihood of each outcome based on the frequency count of the features. The probabilities are then estimated for the new instance using these counts. It can handle the frequency of occurrences of outcomes in a dataset and is particularly useful for text classification.\nBernoulli Naive Bayes: Bernoulli is suitable for datasets where features are binary or boolean, such as text classification where the presence or absence of a feature is more informative than frequency counts. It works similarly to the Multinomial Naive Bayes but with binary variables. It uses the Bernoulli distribution and assumes all our features are binary such that they take only two values.\n\nimport pandas as pd\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\nAAPL['label'] = (AAPL['close'] &gt; AAPL['close'].shift(1)).astype(int)\n\n\nfrom sklearn.model_selection import train_test_split\n\nAAPL = AAPL.dropna()\n\nX = AAPL.drop(['timestamp', 'label'], axis=1)\ny = AAPL['label']\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nSplit data into train and test to ensure that the model can be tuned and evaluated properly. The training set creates the model, the validation set tunes the model’s hyperparameters, and the testing set provides a final metric of how well the model is expected to perform on unseen data.\n\n\n\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\ngnb = GaussianNB()\nmodel = gnb.fit(X_train, y_train)\n\nval_predictions = model.predict(X_val)\nval_accuracy = accuracy_score(y_val, val_predictions)\n\nprint(f\"Validation Accuracy: {val_accuracy}\")\nprint(classification_report(y_val, val_predictions))\n\ntest_predictions = model.predict(X_test)\ntest_accuracy = accuracy_score(y_test, test_predictions)\n\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(classification_report(y_test, test_predictions))\n\nconfusion_mat = confusion_matrix(y_test, test_predictions)\nprint(confusion_mat)\n\nValidation Accuracy: 0.6666666666666666\n              precision    recall  f1-score   support\n\n           0       0.70      0.94      0.80        17\n           1       0.00      0.00      0.00         7\n\n    accuracy                           0.67        24\n   macro avg       0.35      0.47      0.40        24\nweighted avg       0.49      0.67      0.57        24\n\nTest Accuracy: 0.52\n              precision    recall  f1-score   support\n\n           0       0.52      0.92      0.67        13\n           1       0.50      0.08      0.14        12\n\n    accuracy                           0.52        25\n   macro avg       0.51      0.50      0.40        25\nweighted avg       0.51      0.52      0.42        25\n\n[[12  1]\n [11  1]]\n\n\nValidation accuracy is approximately 66.67%, which is the proportion of total correct predictions in the validation set. Test accuracy is 52%, which is the proportion of total correct predictions in the test set.\nFor class 0, Precision is 0.52, meaning when it predicts the negative class, it is correct 52% of the time. Recall is 0.92, meaning it correctly identifies 92% of all true negative cases. F1-score is 0.67, which is a weighted average of precision and recall for the negative class, indicating a relatively moderate performance for this class.\nFor class 1, Precision is 0.50, indicating a coin flip’s chance of being correct when predicting the positive class. Recall is very low at 0.08, meaning it correctly identifies 8% of all true positive cases, which is quite poor. F1-score is 0.14, indicating poor performance for the positive class.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, test_predictions) \nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Test Set')\nplt.show()\n\n\n\n\nValidation Set Performance: The model has an accuracy of 60% on the validation set. This means that 60% of the validation set predictions were correct.\nTest Set Performance: The model has a lower accuracy of 44% on the test set. This decrease in accuracy from the validation set suggests that the model may not generalize well to unseen data.\nIn general, the model is performing poorly, particularly in identifying class 1 instances. This might be due to class imbalance, lack of sufficient predictive features, or model inappropriateness. To improve this, I am trying to find out some other datasets that could help generate a better model and becomes good features."
  },
  {
    "objectID": "Naïve Bayes.html#introduction-to-naive-bayes",
    "href": "Naïve Bayes.html#introduction-to-naive-bayes",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a probabilistic machine learning algorithm based on the Bayes’ Theorem, used for classification tasks. it assumes that features in a dataset are mutually independent, which is a strong assumption.\nFor each class and feature, the method calculates the mean and variance, which will be used to make probability estimates based on the Gaussian distribution. For each instance to be classified, we calculate the posterior probability for every class using the Bayes formula. Assume it is independence between features. So, for multiple features, the posterior probability is the product of individual probabilities of each feature. Then we compare the posterior probabilities across classes and assign the class with the highest probability to the instance, and the class with highest probability is our prediction\nThere are three types of Naive Bayes Classifier: Gaussian, Multinomial, Bernoulli. They share common principles but differ in handling the type of data they are applied to.\nGaussian Naive Bayes: Gaussian is best used in cases where features are continuous and can be assumed to have a Gaussian distribution. It assumes that features follow a normal distribution. Instead of using discrete counts, it uses the mean and variance of the features to estimate the probabilities. If the features are continuous, it assumes that these features are sampled from a Gaussian distribution (bell curve).\nMultinomial Naive Bayes: Multinomial is primarily used for document classification problems where the features are related to word counts or frequencies within the documents. This model is based on frequency counts. It calculates the likelihood of each outcome based on the frequency count of the features. The probabilities are then estimated for the new instance using these counts. It can handle the frequency of occurrences of outcomes in a dataset and is particularly useful for text classification.\nBernoulli Naive Bayes: Bernoulli is suitable for datasets where features are binary or boolean, such as text classification where the presence or absence of a feature is more informative than frequency counts. It works similarly to the Multinomial Naive Bayes but with binary variables. It uses the Bernoulli distribution and assumes all our features are binary such that they take only two values.\n\nimport pandas as pd\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\nAAPL['label'] = (AAPL['close'] &gt; AAPL['close'].shift(1)).astype(int)\n\n\nfrom sklearn.model_selection import train_test_split\n\nAAPL = AAPL.dropna()\n\nX = AAPL.drop(['timestamp', 'label'], axis=1)\ny = AAPL['label']\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nSplit data into train and test to ensure that the model can be tuned and evaluated properly. The training set creates the model, the validation set tunes the model’s hyperparameters, and the testing set provides a final metric of how well the model is expected to perform on unseen data."
  },
  {
    "objectID": "Naïve Bayes.html#naïve-bayes-nb-with-labeled-record-data",
    "href": "Naïve Bayes.html#naïve-bayes-nb-with-labeled-record-data",
    "title": "Naïve Bayes",
    "section": "",
    "text": "from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\ngnb = GaussianNB()\nmodel = gnb.fit(X_train, y_train)\n\nval_predictions = model.predict(X_val)\nval_accuracy = accuracy_score(y_val, val_predictions)\n\nprint(f\"Validation Accuracy: {val_accuracy}\")\nprint(classification_report(y_val, val_predictions))\n\ntest_predictions = model.predict(X_test)\ntest_accuracy = accuracy_score(y_test, test_predictions)\n\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(classification_report(y_test, test_predictions))\n\nconfusion_mat = confusion_matrix(y_test, test_predictions)\nprint(confusion_mat)\n\nValidation Accuracy: 0.6666666666666666\n              precision    recall  f1-score   support\n\n           0       0.70      0.94      0.80        17\n           1       0.00      0.00      0.00         7\n\n    accuracy                           0.67        24\n   macro avg       0.35      0.47      0.40        24\nweighted avg       0.49      0.67      0.57        24\n\nTest Accuracy: 0.52\n              precision    recall  f1-score   support\n\n           0       0.52      0.92      0.67        13\n           1       0.50      0.08      0.14        12\n\n    accuracy                           0.52        25\n   macro avg       0.51      0.50      0.40        25\nweighted avg       0.51      0.52      0.42        25\n\n[[12  1]\n [11  1]]\n\n\nValidation accuracy is approximately 66.67%, which is the proportion of total correct predictions in the validation set. Test accuracy is 52%, which is the proportion of total correct predictions in the test set.\nFor class 0, Precision is 0.52, meaning when it predicts the negative class, it is correct 52% of the time. Recall is 0.92, meaning it correctly identifies 92% of all true negative cases. F1-score is 0.67, which is a weighted average of precision and recall for the negative class, indicating a relatively moderate performance for this class.\nFor class 1, Precision is 0.50, indicating a coin flip’s chance of being correct when predicting the positive class. Recall is very low at 0.08, meaning it correctly identifies 8% of all true positive cases, which is quite poor. F1-score is 0.14, indicating poor performance for the positive class.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, test_predictions) \nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Test Set')\nplt.show()\n\n\n\n\nValidation Set Performance: The model has an accuracy of 60% on the validation set. This means that 60% of the validation set predictions were correct.\nTest Set Performance: The model has a lower accuracy of 44% on the test set. This decrease in accuracy from the validation set suggests that the model may not generalize well to unseen data.\nIn general, the model is performing poorly, particularly in identifying class 1 instances. This might be due to class imbalance, lack of sufficient predictive features, or model inappropriateness. To improve this, I am trying to find out some other datasets that could help generate a better model and becomes good features."
  },
  {
    "objectID": "data cleaning.html",
    "href": "data cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data Cleaning\nIn this section, we will check the raw data and clean the data that we gathered for our analysis.\nFirst, we need to check null values for each datasets. As we can see, the data we gain from this API do not have any Null values, then we need to calculate daily return for each row, to make the dataset not time-sensitive.\n\nimport pandas as pd\n\n#For AAPL dataset\nAAPL = pd.read_csv('AAPL_six_months_data.csv')\n\n#Check for missing values\nmissing_values = AAPL.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(AAPL.dtypes)\n\n#Calculate daily return\nAAPL['daily_return'] = AAPL['close'].pct_change() * 100\nAAPL.head()\n\nAAPL.to_csv('AAPL_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For MSFT dataset\nMSFT = pd.read_csv('MSFT_six_months_data.csv')\n\n#Check for missing values\nmissing_values = MSFT.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(MSFT.dtypes)\n\n#Calculate daily return\nMSFT['daily_return'] = MSFT['close'].pct_change() * 100\nMSFT.head()\n\nMSFT.to_csv('MSFT_Cleaned.csv', index=False)\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For GOOGL dataset\nGOOGL = pd.read_csv('GOOGL_six_months_data.csv')\n\n#Check for missing values\nmissing_values = GOOGL.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(GOOGL.dtypes)\n\n#Calculate daily return\nGOOGL['daily_return'] = GOOGL['close'].pct_change() * 100\nGOOGL.head()\n\nGOOGL.to_csv('GOOGL_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For AMZN dataset\nAMZN = pd.read_csv('AMZN_six_months_data.csv')\n\n#Check for missing values\nmissing_values = AMZN.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(AMZN.dtypes)\n\n#Calculate daily return\nAMZN['daily_return'] = AMZN['close'].pct_change() * 100\nAMZN.head()\n\nAMZN.to_csv('AMZN_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For META dataset\nMETA = pd.read_csv('META_six_months_data.csv')\n\n#Check for missing values\nmissing_values = META.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(META.dtypes)\n\n#Calculate daily return\nMETA['daily_return'] = META['close'].pct_change() * 100\nMETA.head()\n\nMETA.to_csv('META_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For TSLA dataset\nTSLA = pd.read_csv('TSLA_six_months_data.csv')\n\n#Check for missing values\nmissing_values = TSLA.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(TSLA.dtypes)\n\n#Calculate daily return\nTSLA['daily_return'] = TSLA['close'].pct_change() * 100\nTSLA.head()\n\nTSLA.to_csv('TSLA_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object"
  },
  {
    "objectID": "data gathering.html",
    "href": "data gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "In this section, we gather the data that we will be using for this analysis. The data all comes from free API website Alphavantage.\n\n\n\nInitially, we will source our primary dataset from Alpha Vantage, focusing on the stock prices of six prominent high-tech companies – “AAPL, MSFT, GOOGL, AMZN, META, TSLA”, over the recent half-year period. These datasets serve as the base for our exploratory analysis, enabling us to look at the current market trends and behaviors specific to the technology sector.\nTo get the access of these datasets, we need to get our API key from AlphaVantage, then get the authentication of accessing the data.\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\napi_key = 'PMPL4GDC4VRLN6XJ'\n\ndef get_stock_data(symbol, api_key):\n    \n    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}&datatype=csv'\n    r = requests.get(url)\n    if r.status_code == 200:\n        \n        df = pd.read_csv(url)\n        \n        df['timestamp'] = pd.to_datetime(df['timestamp'])\n        \n        six_months_ago = datetime.now() - timedelta(days=6*30)  \n        filtered_df = df[df['timestamp'] &gt;= six_months_ago]\n        return filtered_df\n    else:\n        print(f\"Error fetching data: {r.status_code}\")\n        return None\n\nstock_symbol = 'AAPL'\ndata = get_stock_data(stock_symbol, api_key)\n\nif data is not None:\n    \n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\n\nstock_symbol = 'MSFT'\ndata = get_stock_data(stock_symbol, api_key)\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'GOOGL'\ndata = get_stock_data(stock_symbol, api_key)\n\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'AMZN'\ndata = get_stock_data(stock_symbol, api_key)\n\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'META'\ndata = get_stock_data(stock_symbol, api_key)\n\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'TSLA'\ndata = get_stock_data(stock_symbol, api_key)\n\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nData stored for AAPL for the recent 6 months.\nData stored for MSFT for the recent 6 months.\nData stored for GOOGL for the recent 6 months.\nData stored for AMZN for the recent 6 months.\nData stored for META for the recent 6 months.\nData stored for TSLA for the recent 6 months.\n\n\nWe will also look at the real market trend in the United States, thus we also need the data for real gdp per capita to represent the current market trend.\n\nurl = f'https://www.alphavantage.co/query?function=REAL_GDP_PER_CAPITA&outputsize=full&apikey={api_key}&datatype=csv'\nr = requests.get(url)\nif r.status_code == 200:\n        \n    df = pd.read_csv(url)\n    \n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n        \n    six_months_ago = datetime.now() - timedelta(days=6*30)  \n    filtered_df = df[df['timestamp'] &gt;= six_months_ago]\nelse:\n    print(f\"Error fetching data: {r.status_code}\")\n\nif data is not None:\n\n    data.to_csv(f'Real_GDP_PER_CAPITA_six_months_data.csv', index=False)\n    print(f\"Data stored for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nKeyError: 'timestamp'\n\n\nThe last dataset we need is the news for Apple, which we will use to find out the relationship between news or events to stock price in the future.\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\napi_key = 'PMPL4GDC4VRLN6XJ'\nticker = 'AAPL'\nbase_url = 'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={ticker}&apikey={api_key}'\n\nall_data = []\nstart_date = datetime(2023, 5, 1) \n\nfor _ in range(20):  \n    time_from = start_date.strftime('%Y%m%dT%H%M')\n    url = f\"{base_url}&time_from={time_from}&limit=1000\"  # If limit=50 is the max allowed\n    r = requests.get(url)\n    data = r.json()\n    feed_data = data.get('feed', [])\n    \n    # Break the loop if no data is returned\n    if not feed_data:\n        break\n    \n    # Add the retrieved data to the all_data list\n    all_data.extend(feed_data)\n    \n    # Assuming the feed is ordered by time, update the start_date to the time of the last article\n    last_article_time = feed_data[-1]['time_published']\n    start_date = datetime.strptime(last_article_time, '%Y-%m-%dT%H:%M:%SZ') + timedelta(seconds=1)\n\n# Convert the collected data to a DataFrame\ndf = pd.DataFrame(all_data)\n\n# Save the DataFrame to a CSV file\ndf.to_csv(\"News_AAPL.csv\", index=False)"
  },
  {
    "objectID": "data gathering.html#overview",
    "href": "data gathering.html#overview",
    "title": "Data Gathering",
    "section": "",
    "text": "In this section, we gather the data that we will be using for this analysis. The data all comes from free API website Alphavantage."
  },
  {
    "objectID": "data gathering.html#api",
    "href": "data gathering.html#api",
    "title": "Data Gathering",
    "section": "",
    "text": "Initially, we will source our primary dataset from Alpha Vantage, focusing on the stock prices of six prominent high-tech companies – “AAPL, MSFT, GOOGL, AMZN, META, TSLA”, over the recent half-year period. These datasets serve as the base for our exploratory analysis, enabling us to look at the current market trends and behaviors specific to the technology sector.\nTo get the access of these datasets, we need to get our API key from AlphaVantage, then get the authentication of accessing the data.\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\napi_key = 'PMPL4GDC4VRLN6XJ'\n\ndef get_stock_data(symbol, api_key):\n    \n    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}&datatype=csv'\n    r = requests.get(url)\n    if r.status_code == 200:\n        \n        df = pd.read_csv(url)\n        \n        df['timestamp'] = pd.to_datetime(df['timestamp'])\n        \n        six_months_ago = datetime.now() - timedelta(days=6*30)  \n        filtered_df = df[df['timestamp'] &gt;= six_months_ago]\n        return filtered_df\n    else:\n        print(f\"Error fetching data: {r.status_code}\")\n        return None\n\nstock_symbol = 'AAPL'\ndata = get_stock_data(stock_symbol, api_key)\n\nif data is not None:\n    \n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\n\nstock_symbol = 'MSFT'\ndata = get_stock_data(stock_symbol, api_key)\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'GOOGL'\ndata = get_stock_data(stock_symbol, api_key)\n\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'AMZN'\ndata = get_stock_data(stock_symbol, api_key)\n\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'META'\ndata = get_stock_data(stock_symbol, api_key)\n\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'TSLA'\ndata = get_stock_data(stock_symbol, api_key)\n\n\nif data is not None:\n\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nData stored for AAPL for the recent 6 months.\nData stored for MSFT for the recent 6 months.\nData stored for GOOGL for the recent 6 months.\nData stored for AMZN for the recent 6 months.\nData stored for META for the recent 6 months.\nData stored for TSLA for the recent 6 months.\n\n\nWe will also look at the real market trend in the United States, thus we also need the data for real gdp per capita to represent the current market trend.\n\nurl = f'https://www.alphavantage.co/query?function=REAL_GDP_PER_CAPITA&outputsize=full&apikey={api_key}&datatype=csv'\nr = requests.get(url)\nif r.status_code == 200:\n        \n    df = pd.read_csv(url)\n    \n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n        \n    six_months_ago = datetime.now() - timedelta(days=6*30)  \n    filtered_df = df[df['timestamp'] &gt;= six_months_ago]\nelse:\n    print(f\"Error fetching data: {r.status_code}\")\n\nif data is not None:\n\n    data.to_csv(f'Real_GDP_PER_CAPITA_six_months_data.csv', index=False)\n    print(f\"Data stored for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nKeyError: 'timestamp'\n\n\nThe last dataset we need is the news for Apple, which we will use to find out the relationship between news or events to stock price in the future.\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\napi_key = 'PMPL4GDC4VRLN6XJ'\nticker = 'AAPL'\nbase_url = 'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={ticker}&apikey={api_key}'\n\nall_data = []\nstart_date = datetime(2023, 5, 1) \n\nfor _ in range(20):  \n    time_from = start_date.strftime('%Y%m%dT%H%M')\n    url = f\"{base_url}&time_from={time_from}&limit=1000\"  # If limit=50 is the max allowed\n    r = requests.get(url)\n    data = r.json()\n    feed_data = data.get('feed', [])\n    \n    # Break the loop if no data is returned\n    if not feed_data:\n        break\n    \n    # Add the retrieved data to the all_data list\n    all_data.extend(feed_data)\n    \n    # Assuming the feed is ordered by time, update the start_date to the time of the last article\n    last_article_time = feed_data[-1]['time_published']\n    start_date = datetime.strptime(last_article_time, '%Y-%m-%dT%H:%M:%SZ') + timedelta(seconds=1)\n\n# Convert the collected data to a DataFrame\ndf = pd.DataFrame(all_data)\n\n# Save the DataFrame to a CSV file\ndf.to_csv(\"News_AAPL.csv\", index=False)"
  }
]