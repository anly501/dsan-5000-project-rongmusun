[
  {
    "objectID": "data gathering.html",
    "href": "data gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=IBM&month=2023-09&interval=60min&outputsize=full&apikey={api_key}&datatype=csv\n\n\n\n\nimport requests\nimport pandas as pd\nfrom io import StringIO \n# Replace 'demo' with your own Alpha Vantage API key\napi_key = 'PMPL4GDC4VRLN6XJ'\n\n# Define the API endpoint with datatype=csv\nurl = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=IBM&month=2023-09&interval=60min&outputsize=full&apikey={api_key}&datatype=csv'\n\n# Make the API request\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Get the CSV data\n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    # Process and analyze the CSV data as needed\n\nelse:\n    print(\"Error: Unable to retrieve CSV data from Alpha Vantage API\")\n\ndf.to_csv('../data/sample_py.csv')\n\n\n\n\n\n# Install and load required libraries\nlibrary(httr)\nlibrary(readr)\n\n# Replace 'demo' with your own Alpha Vantage API key\napi_key &lt;- \"PMPL4GDC4VRLN6XJ\"\n\n# Define the API endpoint\nurl &lt;- paste0(\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=IBM&month=2023-09&interval=60min&outputsize=full&apikey=\", api_key, \"&datatype=csv\")\n\n# Make the API request\nresponse &lt;- GET(url)\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n    # Get the CSV data\n    csv_data &lt;- content(response, \"text\", encoding = \"UTF-8\")\n    df &lt;- read_csv(csv_data)\n  \n  # Save the data to CSV\n  write_csv(df, \"../data/sample_r.csv\")\n} else {\n  print(\"Error: Unable to retrieve CSV data from Alpha Vantage API\")\n}\n\nRows: 316 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl  (5): open, high, low, close, volume\ndttm (1): timestamp\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Your Alphavantage API key\napi_key = 'PMPL4GDC4VRLN6XJ'\n\n# Function to fetch the time series data\ndef get_stock_data(symbol, api_key):\n    # Alphavantage URL for the TIME_SERIES_DAILY endpoint\n    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}&datatype=csv'\n    r = requests.get(url)\n    if r.status_code == 200:\n        # Save the CSV content into a DataFrame\n        df = pd.read_csv(url)\n        # Convert the timestamp to datetime and make sure it is in the correct format\n        df['timestamp'] = pd.to_datetime(df['timestamp'])\n        # Filter the data for the last 6 months\n        six_months_ago = datetime.now() - timedelta(days=6*30)  # approximately 6 months\n        filtered_df = df[df['timestamp'] &gt;= six_months_ago]\n        return filtered_df\n    else:\n        print(f\"Error fetching data: {r.status_code}\")\n        return None\n\n# Replace 'IBM' with the symbol of the stock you want to fetch\nstock_symbol = 'AAPL'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\n\nstock_symbol = 'MSFT'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'GOOGL'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'AMZN'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'META'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'TSLA'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nData stored for AAPL for the recent 6 months.\nData stored for MSFT for the recent 6 months.\nData stored for GOOGL for the recent 6 months.\nData stored for AMZN for the recent 6 months.\nData stored for META for the recent 6 months.\nData stored for TSLA for the recent 6 months."
  },
  {
    "objectID": "data gathering.html#link-for-raw-data",
    "href": "data gathering.html#link-for-raw-data",
    "title": "Data Gathering",
    "section": "",
    "text": "https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=IBM&month=2023-09&interval=60min&outputsize=full&apikey={api_key}&datatype=csv"
  },
  {
    "objectID": "data gathering.html#python-api",
    "href": "data gathering.html#python-api",
    "title": "Data Gathering",
    "section": "",
    "text": "import requests\nimport pandas as pd\nfrom io import StringIO \n# Replace 'demo' with your own Alpha Vantage API key\napi_key = 'PMPL4GDC4VRLN6XJ'\n\n# Define the API endpoint with datatype=csv\nurl = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=IBM&month=2023-09&interval=60min&outputsize=full&apikey={api_key}&datatype=csv'\n\n# Make the API request\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Get the CSV data\n    csv_data = response.text\n    df = pd.read_csv(StringIO(csv_data))\n    # Process and analyze the CSV data as needed\n\nelse:\n    print(\"Error: Unable to retrieve CSV data from Alpha Vantage API\")\n\ndf.to_csv('../data/sample_py.csv')"
  },
  {
    "objectID": "data gathering.html#r-api",
    "href": "data gathering.html#r-api",
    "title": "Data Gathering",
    "section": "",
    "text": "# Install and load required libraries\nlibrary(httr)\nlibrary(readr)\n\n# Replace 'demo' with your own Alpha Vantage API key\napi_key &lt;- \"PMPL4GDC4VRLN6XJ\"\n\n# Define the API endpoint\nurl &lt;- paste0(\"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=IBM&month=2023-09&interval=60min&outputsize=full&apikey=\", api_key, \"&datatype=csv\")\n\n# Make the API request\nresponse &lt;- GET(url)\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n    # Get the CSV data\n    csv_data &lt;- content(response, \"text\", encoding = \"UTF-8\")\n    df &lt;- read_csv(csv_data)\n  \n  # Save the data to CSV\n  write_csv(df, \"../data/sample_r.csv\")\n} else {\n  print(\"Error: Unable to retrieve CSV data from Alpha Vantage API\")\n}\n\nRows: 316 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl  (5): open, high, low, close, volume\ndttm (1): timestamp\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nimport requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Your Alphavantage API key\napi_key = 'PMPL4GDC4VRLN6XJ'\n\n# Function to fetch the time series data\ndef get_stock_data(symbol, api_key):\n    # Alphavantage URL for the TIME_SERIES_DAILY endpoint\n    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}&datatype=csv'\n    r = requests.get(url)\n    if r.status_code == 200:\n        # Save the CSV content into a DataFrame\n        df = pd.read_csv(url)\n        # Convert the timestamp to datetime and make sure it is in the correct format\n        df['timestamp'] = pd.to_datetime(df['timestamp'])\n        # Filter the data for the last 6 months\n        six_months_ago = datetime.now() - timedelta(days=6*30)  # approximately 6 months\n        filtered_df = df[df['timestamp'] &gt;= six_months_ago]\n        return filtered_df\n    else:\n        print(f\"Error fetching data: {r.status_code}\")\n        return None\n\n# Replace 'IBM' with the symbol of the stock you want to fetch\nstock_symbol = 'AAPL'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\n\nstock_symbol = 'MSFT'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'GOOGL'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'AMZN'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'META'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nstock_symbol = 'TSLA'\ndata = get_stock_data(stock_symbol, api_key)\n\n# Check if data is returned\nif data is not None:\n    # Store the DataFrame into a CSV file\n    data.to_csv(f'{stock_symbol}_six_months_data.csv', index=False)\n    print(f\"Data stored for {stock_symbol} for the recent 6 months.\")\nelse:\n    print(\"No data to store.\")\n\nData stored for AAPL for the recent 6 months.\nData stored for MSFT for the recent 6 months.\nData stored for GOOGL for the recent 6 months.\nData stored for AMZN for the recent 6 months.\nData stored for META for the recent 6 months.\nData stored for TSLA for the recent 6 months."
  },
  {
    "objectID": "Data Science Topic.html",
    "href": "Data Science Topic.html",
    "title": "DSAN5000-Homework1",
    "section": "",
    "text": "This topic will mainly discuss the performance of each type of content in the social media. The type of content would include videos, images, stories, plain texts, etc. The performance of the content would be measured in several perspectives such as the customer engagement (comments, likes, shares, etc.) or the influence (especially for some brand advertises). By understanding which type of content (or combination of types) performs the best in customer engagement would help brand to increase their advertise influence and maximize the profit brought by their ads. There might be already some researches related to the similar content, but the type of content develops rapidly now and there are many different contents (stories) that still lack of research. For example, instead of limiting categories like videos, pictures, I will separate videos into long videos versus short videos, (videos from tiktok or YouTube). I am assuming that short videos may corresponds to a higher performance as most people do not have the patience to watch a long video, or there might be other content format that ignored by people but performs better."
  },
  {
    "objectID": "Data Science Topic.html#topic-measure-the-content-performance-on-social-media",
    "href": "Data Science Topic.html#topic-measure-the-content-performance-on-social-media",
    "title": "DSAN5000-Homework1",
    "section": "",
    "text": "This topic will mainly discuss the performance of each type of content in the social media. The type of content would include videos, images, stories, plain texts, etc. The performance of the content would be measured in several perspectives such as the customer engagement (comments, likes, shares, etc.) or the influence (especially for some brand advertises). By understanding which type of content (or combination of types) performs the best in customer engagement would help brand to increase their advertise influence and maximize the profit brought by their ads. There might be already some researches related to the similar content, but the type of content develops rapidly now and there are many different contents (stories) that still lack of research. For example, instead of limiting categories like videos, pictures, I will separate videos into long videos versus short videos, (videos from tiktok or YouTube). I am assuming that short videos may corresponds to a higher performance as most people do not have the patience to watch a long video, or there might be other content format that ignored by people but performs better."
  },
  {
    "objectID": "Data Science Topic.html#two-academic-articles",
    "href": "Data Science Topic.html#two-academic-articles",
    "title": "DSAN5000-Homework1",
    "section": "Two academic articles",
    "text": "Two academic articles\n\nImplementing social media marketing strategically: an empirical assessment [gomez2019integrated]\nABSTRACT The purpose of this study is to examine how firms implement social media systematically to drive strategic marketing actions. To this end, the study conceptualises social media implementation as a multidimensional, organisational construct composed of social media strategy, active presence, customer engagement initiatives and social media analytics. Using primary data, the study operationalises the social media implementation construct and tests its effect on firm performance isolated into social media performance and marketing performance. The results indicate that all except the active presence dimension of social media implementation are positively related to social media performance. The results further indicate that social media performance is positively related to marketing performance. The study contributes to the literature by offering a novel conceptualisation and empirical validation of the social media implementation construct.\n\n\nAn integrated model of social media brand engagement [tafesse2018implementing]\nAbstract Despite the increasing use of social media sites to engage consumers, the consumer brand engagement construct is still in its infancy. This study aims to contribute to existing social media research by proposing and empirically testing a model in which social media brand involvement and social media brand communication are the main precursors and brand relationship quality is a relevant outcome of social media brand engagement. The findings show that the influence of social media brand involvement on social media brand engagement is stronger than the influence of social media brand communication. The latter is due to the co-creation of users and firms. Furthermore, interaction and attention are the most relevant components of social media brand engagement, followed by enthusiasm, identification, and absorption. Social media brand engagement is a useful tool for companies to gain competitive advantages. Thus, the findings could help firms better manage their social media tools in the context of social media communication."
  },
  {
    "objectID": "Data Science Topic.html#questions",
    "href": "Data Science Topic.html#questions",
    "title": "DSAN5000-Homework1",
    "section": "Questions:",
    "text": "Questions:\n\nThe popular format of content in social media in 2020s?\nThe classification of different format of content?\nThe model to measure the performance of the content?\nThe best time frame for data? Recent 5 years? 8 years?\nThe data type that could support this research?\nHow to make different formats of content that have different kinds of data to be consistent?\nWhich format would be most popular?\nWhich format is the most effective in customer engagement?\nHow to weight different type of data (number of comment, likes, shares)?\nWhich format performs best in bringing profit for brand or content poster?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Introduction",
    "section": "",
    "text": "Rongmu Sun is a first year graduate student from Georgetown University majoring in DSAN. She recieved her undergraduate degree in George Mason University in BS, Accounting in 2023. She’s new to the DSAN field and now still exploring her interest.\n\n\n\nimage.png\n\n\n\n\n\nBig fan of SEVENTEEN.\nLove Video Games: Apex, Valorant, League of Legend, steam games etc.\nAnimations.\n\n\n\n\nEmail: rs2264@georgetown.edu\nPhone: (571)259-6817\nInstagram: srongmu\nLinkedIn: Rongmu Sun"
  },
  {
    "objectID": "about.html#rongmu-sun",
    "href": "about.html#rongmu-sun",
    "title": "Introduction",
    "section": "",
    "text": "Rongmu Sun is a first year graduate student from Georgetown University majoring in DSAN. She recieved her undergraduate degree in George Mason University in BS, Accounting in 2023. She’s new to the DSAN field and now still exploring her interest.\n\n\n\nimage.png"
  },
  {
    "objectID": "about.html#hobbies-and-interests",
    "href": "about.html#hobbies-and-interests",
    "title": "Introduction",
    "section": "",
    "text": "Big fan of SEVENTEEN.\nLove Video Games: Apex, Valorant, League of Legend, steam games etc.\nAnimations."
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "Introduction",
    "section": "",
    "text": "Email: rs2264@georgetown.edu\nPhone: (571)259-6817\nInstagram: srongmu\nLinkedIn: Rongmu Sun"
  },
  {
    "objectID": "website-template/pages/markdown-cheatsheet/cheatsheet.html",
    "href": "website-template/pages/markdown-cheatsheet/cheatsheet.html",
    "title": "Markdown Cheat Sheet",
    "section": "",
    "text": "Thanks for visiting The Markdown Guide!\nThis Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for basic syntax and extended syntax.\n\n\nThese are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements."
  },
  {
    "objectID": "website-template/pages/markdown-cheatsheet/cheatsheet.html#basic-syntax",
    "href": "website-template/pages/markdown-cheatsheet/cheatsheet.html#basic-syntax",
    "title": "Markdown Cheat Sheet",
    "section": "",
    "text": "These are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements."
  },
  {
    "objectID": "website-template/pages/markdown-cheatsheet/cheatsheet.html#h2",
    "href": "website-template/pages/markdown-cheatsheet/cheatsheet.html#h2",
    "title": "Markdown Cheat Sheet",
    "section": "H2",
    "text": "H2\n\nH3\n\n\nBold\nbold text\n\n\nItalic\nitalicized text\n\n\nBlockquote\n\nblockquote\n\n\n\nOrdered List\n\nFirst item\nSecond item\nThird item\n\n\n\nUnordered List\n\nFirst item\nSecond item\nThird item\n\n\n\nCode\ncode\n\n\nHorizontal Rule\n\n\n\nLink\nMarkdown Guide\n\n\nImage\nIMAGE ON YOUR COMPUTER (RELATIVE PATH)\n\n\n\nalt text\n\n\nIMAGE FROM ONLINE\n\n\n\nalt text"
  },
  {
    "objectID": "website-template/pages/markdown-cheatsheet/cheatsheet.html#extended-syntax",
    "href": "website-template/pages/markdown-cheatsheet/cheatsheet.html#extended-syntax",
    "title": "Markdown Cheat Sheet",
    "section": "Extended Syntax",
    "text": "Extended Syntax\nThese elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements.\n\nTable\n\n\n\nSyntax\nDescription\n\n\n\n\nHeader\nTitle\n\n\nParagraph\nText\n\n\n\n\n\nFenced Code Block\n{\n  \"firstName\": \"John\",\n  \"lastName\": \"Smith\",\n  \"age\": 25\n}\n\n\nFootnote\nHere’s a sentence with a footnote. 1\n\n\nHeading ID\n\n\nMy Great Heading\n\n\nDefinition List\n\nterm\n\ndefinition\n\n\n\n\nStrikethrough\nThe world is flat.\n\n\nTask List\n\nWrite the press release\nUpdate the website\nContact the media\n\n\n\nEmoji\nThat is so funny! :joy:\n(See also Copying and Pasting Emoji)\n\n\nHighlight\nI need to highlight these ==very important words==.\n\n\nSubscript\nH2O\n\n\nSuperscript\nX2"
  },
  {
    "objectID": "website-template/pages/markdown-cheatsheet/cheatsheet.html#footnotes",
    "href": "website-template/pages/markdown-cheatsheet/cheatsheet.html#footnotes",
    "title": "Markdown Cheat Sheet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the footnote.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homework 1",
    "section": "",
    "text": "This is the Homework 1 of DSAN 5000.\nThis site includes a brief self-introduction page and a Data Science Topic page.\nTo learn more about this project visit github https://github.com/anly501/dsan-5000-project-rongmusun."
  },
  {
    "objectID": "Naïve Bayes.html",
    "href": "Naïve Bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a probabilistic machine learning algorithm based on the Bayes’ Theorem, used for classification tasks. it assumes that features in a dataset are mutually independent, which is a strong assumption.\nFor each class and feature, the method calculates the mean and variance, which will be used to make probability estimates based on the Gaussian distribution. For each instance to be classified, we calculate the posterior probability for every class using the Bayes formula. Assume it is independence between features. So, for multiple features, the posterior probability is the product of individual probabilities of each feature. Then we compare the posterior probabilities across classes and assign the class with the highest probability to the instance, and the class with highest probability is our prediction\nThere are three types of Naive Bayes Classifier: Gaussian, Multinomial, Bernoulli. They share common principles but differ in handling the type of data they are applied to.\nGaussian Naive Bayes: Gaussian is best used in cases where features are continuous and can be assumed to have a Gaussian distribution. It assumes that features follow a normal distribution. Instead of using discrete counts, it uses the mean and variance of the features to estimate the probabilities. If the features are continuous, it assumes that these features are sampled from a Gaussian distribution (bell curve).\nMultinomial Naive Bayes: Multinomial is primarily used for document classification problems where the features are related to word counts or frequencies within the documents. This model is based on frequency counts. It calculates the likelihood of each outcome based on the frequency count of the features. The probabilities are then estimated for the new instance using these counts. It can handle the frequency of occurrences of outcomes in a dataset and is particularly useful for text classification.\nBernoulli Naive Bayes: Bernoulli is suitable for datasets where features are binary or boolean, such as text classification where the presence or absence of a feature is more informative than frequency counts. It works similarly to the Multinomial Naive Bayes but with binary variables. It uses the Bernoulli distribution and assumes all our features are binary such that they take only two values.\n\nimport requests\n\napi_key = '4ce9d5ba9ae046d8b647f0993345ef7f'\n\nurl = 'https://newsapi.org/v2/everything'\nparams = {\n    'q': 'Apple', \n    'from': '2023-05-08', \n    'to': '2023-11-01', \n    'sortBy': 'publishedAt', \n    'apiKey': api_key, \n}\nresponse = requests.get(url, params=params)\n\nif response.status_code == 200:\n    data = response.json()\n    articles = data['articles']\n    df_articles = pd.DataFrame(articles)\n    print(df_articles.head())\nelse:\n    print(f\"Failed to fetch news: {response.status_code}\")\n\ncolumns_of_interest = ['source', 'author', 'title', 'description', 'url', 'publishedAt', 'content']\ndf_articles_filtered = df_articles[columns_of_interest]\nprint(df_articles_filtered.head())\n\ndf_articles_filtered.to_csv(\"AAPL_NEWS.csv\")\n\nFailed to fetch news: 426\n                                              source  \\\n0                 {'id': None, 'name': 'Biztoc.com'}   \n1  {'id': None, 'name': 'Investor's Business Daily'}   \n2               {'id': None, 'name': 'Applech2.com'}   \n3               {'id': None, 'name': 'Applech2.com'}   \n4               {'id': None, 'name': 'Applech2.com'}   \n\n                      author  \\\n0              thestreet.com   \n1  Investor's Business Daily   \n2                   applech2   \n3                   applech2   \n4                   applech2   \n\n                                               title  \\\n0  One tech startup has found a solution to in-pe...   \n1  Dow Jones Futures: What To Do After Big Market...   \n2  AmazonでSatechiのMacBook Pro (14/16インチ)用ハードシェルケー...   \n3  macOS 14.1 SonomaではWindow Serverのバグにより、Adobe P...   \n4  エレコム、macOS 14 Sonomaでジェスチャー機能を割り当てした際、アプリが強制終了...   \n\n                                         description  \\\n0  As much as 2021 was synonymous with an enormou...   \n1  Warren Buffett's Berkshire reported strong ear...   \n2  AmazonでがSatechiのMacBook Pro (14/16インチ)用ハードシェルケ...   \n3  macOS 14.1 SonomaではWindow Serverのバグにより、Adobe P...   \n4  エレコムがmacOS 14 Sonomaでジェスチャー機能を割り当てした際、アプリが強制終了...   \n\n                                                 url           publishedAt  \\\n0              https://biztoc.com/x/9a75cf0635c15ca3  2023-11-05T13:10:15Z   \n1  https://www.investors.com/market-trend/stock-m...  2023-11-05T12:44:39Z   \n2  https://applech2.com/archives/20231105-satechi...  2023-11-05T09:13:08Z   \n3  https://applech2.com/archives/20231105-macos-1...  2023-11-05T08:43:57Z   \n4  https://applech2.com/archives/20231105-elecom-...  2023-11-05T07:02:58Z   \n\n                                             content  \n0  As much as 2021 was synonymous with an enormou...  \n1  Dow Jones futures will open Sunday evening, al...  \n2  AmazonSatechiMacBook Pro (14/16)iMac 24\\r\\nAma...  \n3  macOS 14.1 SonomaWindow ServerAdobe PhotoshopI...  \n4  macOS 14 Sonoma v5.2.12.002\\r\\n20231027 (Mac)v...  \n\n\n\nimport pandas as pd\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\nMSFT = pd.read_csv('MSFT_Cleaned.csv')\nGOOGL = pd.read_csv('GOOGL_Cleaned.csv')\nAMZN = pd.read_csv('AMZN_Cleaned.csv')\nMETA = pd.read_csv('META_Cleaned.csv')\nTSLA = pd.read_csv('TSLA_Cleaned.csv')\n\n\nAAPL['label'] = (AAPL['close'] &gt; AAPL['close'].shift(1)).astype(int)\nMSFT['label'] = (MSFT['close'] &gt; MSFT['close'].shift(1)).astype(int)\nGOOGL['label'] = (GOOGL['close'] &gt; GOOGL['close'].shift(1)).astype(int)\nAMZN['label'] = (AMZN['close'] &gt; AMZN['close'].shift(1)).astype(int)\nMETA['label'] = (META['close'] &gt; META['close'].shift(1)).astype(int)\nTSLA['label'] = (TSLA['close'] &gt; TSLA['close'].shift(1)).astype(int)\n\n\nfrom sklearn.model_selection import train_test_split\n\nAAPL = AAPL.dropna()\n\nX = AAPL.drop(['timestamp', 'label'], axis=1)\ny = AAPL['label']\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nSplit data into train and test to ensure that the model can be tuned and evaluated properly. The training set creates the model, the validation set tunes the model’s hyperparameters, and the testing set provides a final metric of how well the model is expected to perform on unseen data.\n\n\n\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nAAPL = AAPL.drop('timestamp', axis=1)\nAAPL = AAPL.dropna()\n\nX = AAPL.drop('close', axis=1)\ny = AAPL['close']\n\nmodel = LinearRegression()\nrfe = RFE(estimator=model, n_features_to_select=3) \nselector = rfe.fit(X, y)\n\nselected_features = feature_names[selector.support_]\nprint(\"Selected features:\")\nprint(selected_features)\n\nSelected features:\nIndex(['open', 'high', 'low'], dtype='object')\n\n\n\n\n\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\ngnb = GaussianNB()\nmodel = gnb.fit(X_train, y_train)\n\nval_predictions = model.predict(X_val)\nval_accuracy = accuracy_score(y_val, val_predictions)\n\nprint(f\"Validation Accuracy: {val_accuracy}\")\nprint(classification_report(y_val, val_predictions))\n\ntest_predictions = model.predict(X_test)\ntest_accuracy = accuracy_score(y_test, test_predictions)\n\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(classification_report(y_test, test_predictions))\n\nconfusion_mat = confusion_matrix(y_test, test_predictions)\nprint(confusion_mat)\n\nValidation Accuracy: 0.6\n              precision    recall  f1-score   support\n\n           0       0.62      0.94      0.75        16\n           1       0.00      0.00      0.00         9\n\n    accuracy                           0.60        25\n   macro avg       0.31      0.47      0.38        25\nweighted avg       0.40      0.60      0.48        25\n\nTest Accuracy: 0.44\n              precision    recall  f1-score   support\n\n           0       0.44      1.00      0.61        11\n           1       0.00      0.00      0.00        14\n\n    accuracy                           0.44        25\n   macro avg       0.22      0.50      0.31        25\nweighted avg       0.19      0.44      0.27        25\n\n[[11  0]\n [14  0]]\n\n\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, test_predictions) \nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Test Set')\nplt.show()\n\n\n\n\nValidation Set Performance: The model has an accuracy of 60% on the validation set. This means that 60% of the validation set predictions were correct.\nTest Set Performance: The model has a lower accuracy of 44% on the test set. This decrease in accuracy from the validation set suggests that the model may not generalize well to unseen data.\nIn general, the model is performing poorly, particularly in identifying class 1 instances. This might be due to class imbalance, lack of sufficient predictive features, or model inappropriateness. To improve this, I am trying to find out some other datasets that could help generate a better model and becomes good features."
  },
  {
    "objectID": "Naïve Bayes.html#introduction-to-naive-bayes",
    "href": "Naïve Bayes.html#introduction-to-naive-bayes",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes is a probabilistic machine learning algorithm based on the Bayes’ Theorem, used for classification tasks. it assumes that features in a dataset are mutually independent, which is a strong assumption.\nFor each class and feature, the method calculates the mean and variance, which will be used to make probability estimates based on the Gaussian distribution. For each instance to be classified, we calculate the posterior probability for every class using the Bayes formula. Assume it is independence between features. So, for multiple features, the posterior probability is the product of individual probabilities of each feature. Then we compare the posterior probabilities across classes and assign the class with the highest probability to the instance, and the class with highest probability is our prediction\nThere are three types of Naive Bayes Classifier: Gaussian, Multinomial, Bernoulli. They share common principles but differ in handling the type of data they are applied to.\nGaussian Naive Bayes: Gaussian is best used in cases where features are continuous and can be assumed to have a Gaussian distribution. It assumes that features follow a normal distribution. Instead of using discrete counts, it uses the mean and variance of the features to estimate the probabilities. If the features are continuous, it assumes that these features are sampled from a Gaussian distribution (bell curve).\nMultinomial Naive Bayes: Multinomial is primarily used for document classification problems where the features are related to word counts or frequencies within the documents. This model is based on frequency counts. It calculates the likelihood of each outcome based on the frequency count of the features. The probabilities are then estimated for the new instance using these counts. It can handle the frequency of occurrences of outcomes in a dataset and is particularly useful for text classification.\nBernoulli Naive Bayes: Bernoulli is suitable for datasets where features are binary or boolean, such as text classification where the presence or absence of a feature is more informative than frequency counts. It works similarly to the Multinomial Naive Bayes but with binary variables. It uses the Bernoulli distribution and assumes all our features are binary such that they take only two values.\n\nimport requests\n\napi_key = '4ce9d5ba9ae046d8b647f0993345ef7f'\n\nurl = 'https://newsapi.org/v2/everything'\nparams = {\n    'q': 'Apple', \n    'from': '2023-05-08', \n    'to': '2023-11-01', \n    'sortBy': 'publishedAt', \n    'apiKey': api_key, \n}\nresponse = requests.get(url, params=params)\n\nif response.status_code == 200:\n    data = response.json()\n    articles = data['articles']\n    df_articles = pd.DataFrame(articles)\n    print(df_articles.head())\nelse:\n    print(f\"Failed to fetch news: {response.status_code}\")\n\ncolumns_of_interest = ['source', 'author', 'title', 'description', 'url', 'publishedAt', 'content']\ndf_articles_filtered = df_articles[columns_of_interest]\nprint(df_articles_filtered.head())\n\ndf_articles_filtered.to_csv(\"AAPL_NEWS.csv\")\n\nFailed to fetch news: 426\n                                              source  \\\n0                 {'id': None, 'name': 'Biztoc.com'}   \n1  {'id': None, 'name': 'Investor's Business Daily'}   \n2               {'id': None, 'name': 'Applech2.com'}   \n3               {'id': None, 'name': 'Applech2.com'}   \n4               {'id': None, 'name': 'Applech2.com'}   \n\n                      author  \\\n0              thestreet.com   \n1  Investor's Business Daily   \n2                   applech2   \n3                   applech2   \n4                   applech2   \n\n                                               title  \\\n0  One tech startup has found a solution to in-pe...   \n1  Dow Jones Futures: What To Do After Big Market...   \n2  AmazonでSatechiのMacBook Pro (14/16インチ)用ハードシェルケー...   \n3  macOS 14.1 SonomaではWindow Serverのバグにより、Adobe P...   \n4  エレコム、macOS 14 Sonomaでジェスチャー機能を割り当てした際、アプリが強制終了...   \n\n                                         description  \\\n0  As much as 2021 was synonymous with an enormou...   \n1  Warren Buffett's Berkshire reported strong ear...   \n2  AmazonでがSatechiのMacBook Pro (14/16インチ)用ハードシェルケ...   \n3  macOS 14.1 SonomaではWindow Serverのバグにより、Adobe P...   \n4  エレコムがmacOS 14 Sonomaでジェスチャー機能を割り当てした際、アプリが強制終了...   \n\n                                                 url           publishedAt  \\\n0              https://biztoc.com/x/9a75cf0635c15ca3  2023-11-05T13:10:15Z   \n1  https://www.investors.com/market-trend/stock-m...  2023-11-05T12:44:39Z   \n2  https://applech2.com/archives/20231105-satechi...  2023-11-05T09:13:08Z   \n3  https://applech2.com/archives/20231105-macos-1...  2023-11-05T08:43:57Z   \n4  https://applech2.com/archives/20231105-elecom-...  2023-11-05T07:02:58Z   \n\n                                             content  \n0  As much as 2021 was synonymous with an enormou...  \n1  Dow Jones futures will open Sunday evening, al...  \n2  AmazonSatechiMacBook Pro (14/16)iMac 24\\r\\nAma...  \n3  macOS 14.1 SonomaWindow ServerAdobe PhotoshopI...  \n4  macOS 14 Sonoma v5.2.12.002\\r\\n20231027 (Mac)v...  \n\n\n\nimport pandas as pd\n\nAAPL = pd.read_csv('AAPL_Cleaned.csv')\nMSFT = pd.read_csv('MSFT_Cleaned.csv')\nGOOGL = pd.read_csv('GOOGL_Cleaned.csv')\nAMZN = pd.read_csv('AMZN_Cleaned.csv')\nMETA = pd.read_csv('META_Cleaned.csv')\nTSLA = pd.read_csv('TSLA_Cleaned.csv')\n\n\nAAPL['label'] = (AAPL['close'] &gt; AAPL['close'].shift(1)).astype(int)\nMSFT['label'] = (MSFT['close'] &gt; MSFT['close'].shift(1)).astype(int)\nGOOGL['label'] = (GOOGL['close'] &gt; GOOGL['close'].shift(1)).astype(int)\nAMZN['label'] = (AMZN['close'] &gt; AMZN['close'].shift(1)).astype(int)\nMETA['label'] = (META['close'] &gt; META['close'].shift(1)).astype(int)\nTSLA['label'] = (TSLA['close'] &gt; TSLA['close'].shift(1)).astype(int)\n\n\nfrom sklearn.model_selection import train_test_split\n\nAAPL = AAPL.dropna()\n\nX = AAPL.drop(['timestamp', 'label'], axis=1)\ny = AAPL['label']\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nSplit data into train and test to ensure that the model can be tuned and evaluated properly. The training set creates the model, the validation set tunes the model’s hyperparameters, and the testing set provides a final metric of how well the model is expected to perform on unseen data."
  },
  {
    "objectID": "Naïve Bayes.html#feature-selection-for-record-data",
    "href": "Naïve Bayes.html#feature-selection-for-record-data",
    "title": "Naïve Bayes",
    "section": "",
    "text": "from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nAAPL = AAPL.drop('timestamp', axis=1)\nAAPL = AAPL.dropna()\n\nX = AAPL.drop('close', axis=1)\ny = AAPL['close']\n\nmodel = LinearRegression()\nrfe = RFE(estimator=model, n_features_to_select=3) \nselector = rfe.fit(X, y)\n\nselected_features = feature_names[selector.support_]\nprint(\"Selected features:\")\nprint(selected_features)\n\nSelected features:\nIndex(['open', 'high', 'low'], dtype='object')"
  },
  {
    "objectID": "Naïve Bayes.html#naïve-bayes-nb-with-labeled-record-data",
    "href": "Naïve Bayes.html#naïve-bayes-nb-with-labeled-record-data",
    "title": "Naïve Bayes",
    "section": "",
    "text": "from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\ngnb = GaussianNB()\nmodel = gnb.fit(X_train, y_train)\n\nval_predictions = model.predict(X_val)\nval_accuracy = accuracy_score(y_val, val_predictions)\n\nprint(f\"Validation Accuracy: {val_accuracy}\")\nprint(classification_report(y_val, val_predictions))\n\ntest_predictions = model.predict(X_test)\ntest_accuracy = accuracy_score(y_test, test_predictions)\n\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(classification_report(y_test, test_predictions))\n\nconfusion_mat = confusion_matrix(y_test, test_predictions)\nprint(confusion_mat)\n\nValidation Accuracy: 0.6\n              precision    recall  f1-score   support\n\n           0       0.62      0.94      0.75        16\n           1       0.00      0.00      0.00         9\n\n    accuracy                           0.60        25\n   macro avg       0.31      0.47      0.38        25\nweighted avg       0.40      0.60      0.48        25\n\nTest Accuracy: 0.44\n              precision    recall  f1-score   support\n\n           0       0.44      1.00      0.61        11\n           1       0.00      0.00      0.00        14\n\n    accuracy                           0.44        25\n   macro avg       0.22      0.50      0.31        25\nweighted avg       0.19      0.44      0.27        25\n\n[[11  0]\n [14  0]]\n\n\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/srongmu/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, test_predictions) \nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for Test Set')\nplt.show()\n\n\n\n\nValidation Set Performance: The model has an accuracy of 60% on the validation set. This means that 60% of the validation set predictions were correct.\nTest Set Performance: The model has a lower accuracy of 44% on the test set. This decrease in accuracy from the validation set suggests that the model may not generalize well to unseen data.\nIn general, the model is performing poorly, particularly in identifying class 1 instances. This might be due to class imbalance, lack of sufficient predictive features, or model inappropriateness. To improve this, I am trying to find out some other datasets that could help generate a better model and becomes good features."
  },
  {
    "objectID": "data cleaning.html",
    "href": "data cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import pandas as pd\n\n#For AAPL dataset\nAAPL = pd.read_csv('AAPL_six_months_data.csv')\n\n#Check for missing values\nmissing_values = AAPL.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(AAPL.dtypes)\n\n#Calculate daily return\nAAPL['daily_return'] = AAPL['close'].pct_change() * 100\nAAPL.head()\n\nAAPL.to_csv('AAPL_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For MSFT dataset\nMSFT = pd.read_csv('MSFT_six_months_data.csv')\n\n#Check for missing values\nmissing_values = MSFT.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(MSFT.dtypes)\n\n#Calculate daily return\nMSFT['daily_return'] = MSFT['close'].pct_change() * 100\nMSFT.head()\n\nMSFT.to_csv('MSFT_Cleaned.csv', index=False)\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For GOOGL dataset\nGOOGL = pd.read_csv('GOOGL_six_months_data.csv')\n\n#Check for missing values\nmissing_values = GOOGL.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(GOOGL.dtypes)\n\n#Calculate daily return\nGOOGL['daily_return'] = GOOGL['close'].pct_change() * 100\nGOOGL.head()\n\nGOOGL.to_csv('GOOGL_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For AMZN dataset\nAMZN = pd.read_csv('AMZN_six_months_data.csv')\n\n#Check for missing values\nmissing_values = AMZN.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(AMZN.dtypes)\n\n#Calculate daily return\nAMZN['daily_return'] = AMZN['close'].pct_change() * 100\nAMZN.head()\n\nAMZN.to_csv('AMZN_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For META dataset\nMETA = pd.read_csv('META_six_months_data.csv')\n\n#Check for missing values\nmissing_values = META.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(META.dtypes)\n\n#Calculate daily return\nMETA['daily_return'] = META['close'].pct_change() * 100\nMETA.head()\n\nMETA.to_csv('META_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#For TSLA dataset\nTSLA = pd.read_csv('TSLA_six_months_data.csv')\n\n#Check for missing values\nmissing_values = TSLA.isnull().sum()\nprint(missing_values)\n\n#Check data type\nprint(TSLA.dtypes)\n\n#Calculate daily return\nTSLA['daily_return'] = TSLA['close'].pct_change() * 100\nTSLA.head()\n\nTSLA.to_csv('TSLA_Cleaned.csv')\n\ntimestamp    0\nopen         0\nhigh         0\nlow          0\nclose        0\nvolume       0\ndtype: int64\ntimestamp     object\nopen         float64\nhigh         float64\nlow          float64\nclose        float64\nvolume         int64\ndtype: object\n\n\n\n#Drop volumn column\nAAPL = AAPL.drop('volume', axis=1)\nMSFT = MSFT.drop('volume', axis=1)\nGOOGL = GOOGL.drop('volume', axis=1)\nAMZN = AMZN.drop('volume', axis=1)\nMETA = META.drop('volume', axis=1)\nTSLA = TSLA.drop('volume', axis=1)\n\n\n\n\n\n\n#Summary statistics for Data\n\nsummary_statistics_AAPL = AAPL.describe()\ncolumns_to_include = [col for col in AAPL.columns if col != 'volume']\nsummary_statistics_AAPL = AAPL[columns_to_include].describe().drop('count')\n\nsummary_statistics_MSFT = MSFT.describe()\ncolumns_to_include = [col for col in MSFT.columns if col != 'volume']\nsummary_statistics_MSFT = MSFT[columns_to_include].describe().drop('count')\n\nsummary_statistics_GOOGL = GOOGL.describe()\ncolumns_to_include = [col for col in GOOGL.columns if col != 'volume']\nsummary_statistics_GOOGL = GOOGL[columns_to_include].describe().drop('count')\n\nsummary_statistics_AMZN = AMZN.describe()\ncolumns_to_include = [col for col in AMZN.columns if col != 'volume']\nsummary_statistics_AMZN = AMZN[columns_to_include].describe().drop('count')\n\nsummary_statistics_META = META.describe()\ncolumns_to_include = [col for col in META.columns if col != 'volume']\nsummary_statistics_META = META[columns_to_include].describe().drop('count')\n\nsummary_statistics_TSLA = TSLA.describe()\ncolumns_to_include = [col for col in TSLA.columns if col != 'volume']\nsummary_statistics_TSLA = TSLA[columns_to_include].describe().drop('count')\n\n\nprint(summary_statistics_AAPL)\nprint(summary_statistics_MSFT)\nprint(summary_statistics_GOOGL)\nprint(summary_statistics_AMZN)\nprint(summary_statistics_META)\nprint(summary_statistics_TSLA)\n\n            open        high         low       close  daily_return\nmean  180.497573  182.016635  179.110013  180.585403      0.004905\nstd     7.666419    7.603430    7.683683    7.581467      1.201368\nmin   166.910000  168.960000  165.670000  166.890000     -2.258081\n25%   173.995000  175.690000  173.162500  174.420000     -0.775974\n50%   178.395000  180.120000  177.217500  178.665000     -0.126516\n75%   186.755000  188.135000  185.065000  187.162500      0.710672\nmax   196.235000  198.230000  195.280000  196.450000      5.044233\n            open        high         low       close  daily_return\nmean  329.248129  332.421206  326.081446  329.164395     -0.082502\nstd    10.424783   10.871145    9.758547   10.254954      1.454917\nmin   308.000000  309.905000  306.090000  307.000000     -3.827645\n25%   321.450000  325.357475  319.920875  321.875000     -0.933508\n50%   331.055000  333.430000  327.120000  329.815000     -0.166908\n75%   335.222500  338.450000  332.885000  335.925000      0.741900\nmax   361.750000  366.780000  352.435000  359.490000      3.910945\n            open        high         low       close  daily_return\nmean  127.470927  129.022405  126.258075  127.643468     -0.113071\nstd     7.345721    7.142713    7.210315    7.182428      1.852893\nmin   105.180000  107.960000  105.160000  107.350000     -5.461437\n25%   122.362500  123.976250  121.195000  122.325000     -1.082018\n50%   128.335000  129.317500  126.890000  128.240000     -0.100464\n75%   133.357500  134.687500  131.940000  133.847500      0.848728\nmax   141.050000  141.220000  138.580000  140.550000     10.508717\n            open        high         low       close  daily_return\nmean  128.983661  130.550713  127.459491  129.020242     -0.188504\nstd     7.986347    7.903721    7.828463    7.879146      2.060386\nmin   105.040000  106.095000  104.700100  105.830000     -7.637745\n25%   125.557500  127.363750  124.130000  125.975000     -1.540472\n50%   129.580000  131.252500  128.204700  129.255000     -0.122718\n75%   133.995000  135.627500  132.822500  134.262500      0.913674\nmax   145.080000  145.860000  142.950000  144.850000      5.906582\n            open        high         low       close  daily_return\nmean  291.056734  295.337216  287.171990  291.285524     -0.218422\nstd    23.982296   23.841979   22.977277   23.244562      1.870259\nmin   231.415000  235.620000  230.270000  233.080000     -4.230675\n25%   280.902500  285.582500  277.255000  281.782500     -1.445339\n50%   297.830000  301.090000  293.350000  297.815000     -0.173350\n75%   307.420000  312.084750  301.685000  307.005000      0.877795\nmax   328.000000  330.540000  322.950000  327.820000      4.459209\n            open        high         low       close  daily_return\nmean  240.241921  245.035196  235.691160  240.501855     -0.098334\nstd    31.794378   31.973045   31.228469   31.588256      3.112003\nmin   165.650000  169.518400  164.350000  166.350000     -9.167337\n25%   220.613400  225.567500  216.215000  219.887500     -1.756820\n50%   249.850000  255.394950  244.535000  250.670000     -0.102090\n75%   261.102500  267.400000  257.319325  261.312500      1.699834\nmax   296.040000  299.290000  289.520100  293.340000     10.787372\n\n\n\n\n\n\n#Line plot for closing price for each company\nAAPL['timestamp'] = pd.to_datetime(AAPL['timestamp'])\nAAPL.set_index('timestamp', inplace=True)\nMSFT['timestamp'] = pd.to_datetime(MSFT['timestamp'])\nMSFT.set_index('timestamp', inplace=True)\nGOOGL['timestamp'] = pd.to_datetime(GOOGL['timestamp'])\nGOOGL.set_index('timestamp', inplace=True)\nAMZN['timestamp'] = pd.to_datetime(AMZN['timestamp'])\nAMZN.set_index('timestamp', inplace=True)\nMETA['timestamp'] = pd.to_datetime(META['timestamp'])\nMETA.set_index('timestamp', inplace=True)\nTSLA['timestamp'] = pd.to_datetime(TSLA['timestamp'])\nTSLA.set_index('timestamp', inplace=True)\n\nclosing_prices = pd.DataFrame({\n    'AAPL': AAPL['close'],\n    'MSFT': MSFT['close'],\n    'GOOGL': GOOGL['close'],\n    'AMZN': AMZN['close'],\n    'META': META['close'],\n    'TSLA': TSLA['close']\n})\nplt.figure(figsize=(15, 8))\nfor ticker in closing_prices.columns:\n    plt.plot(closing_prices.index, closing_prices[ticker], label=ticker)\n\nplt.title('Closing Prices Over Time')\nplt.xlabel('Date')\nplt.ylabel('Price (USD)')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Distribution for daily return\nimport seaborn as sns\nfor column in AAPL.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(AAPL[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in MSFT.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(MSFT[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in GOOGL.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(GOOGL[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in AMZN.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(AMZN[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in META.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(META[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in TSLA.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(TSLA[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndaily_returns = closing_prices.pct_change().dropna()\n\nsns.boxplot(data=daily_returns)\n\nplt.title('Box Plot of Daily Returns for Different Stocks')\nplt.ylabel('Daily Return')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\ncorrelation_matrix = daily_returns.corr()\n\nsns.heatmap(correlation_matrix, annot=True, cmap='Blues', linewidths=.5)\nplt.title('Correlation Matrix for Daily Returns of Stocks')\nplt.show()\n\n\n\n\n\n\n\nCan intraday price ranges be used to predict next-day stock volatility?\nHypothesis: There is a positive correlation between the intraday high-low spread and the volatility of the stock on the following day."
  },
  {
    "objectID": "data cleaning.html#eda",
    "href": "data cleaning.html#eda",
    "title": "Data Cleaning",
    "section": "",
    "text": "#Summary statistics for Data\n\nsummary_statistics_AAPL = AAPL.describe()\ncolumns_to_include = [col for col in AAPL.columns if col != 'volume']\nsummary_statistics_AAPL = AAPL[columns_to_include].describe().drop('count')\n\nsummary_statistics_MSFT = MSFT.describe()\ncolumns_to_include = [col for col in MSFT.columns if col != 'volume']\nsummary_statistics_MSFT = MSFT[columns_to_include].describe().drop('count')\n\nsummary_statistics_GOOGL = GOOGL.describe()\ncolumns_to_include = [col for col in GOOGL.columns if col != 'volume']\nsummary_statistics_GOOGL = GOOGL[columns_to_include].describe().drop('count')\n\nsummary_statistics_AMZN = AMZN.describe()\ncolumns_to_include = [col for col in AMZN.columns if col != 'volume']\nsummary_statistics_AMZN = AMZN[columns_to_include].describe().drop('count')\n\nsummary_statistics_META = META.describe()\ncolumns_to_include = [col for col in META.columns if col != 'volume']\nsummary_statistics_META = META[columns_to_include].describe().drop('count')\n\nsummary_statistics_TSLA = TSLA.describe()\ncolumns_to_include = [col for col in TSLA.columns if col != 'volume']\nsummary_statistics_TSLA = TSLA[columns_to_include].describe().drop('count')\n\n\nprint(summary_statistics_AAPL)\nprint(summary_statistics_MSFT)\nprint(summary_statistics_GOOGL)\nprint(summary_statistics_AMZN)\nprint(summary_statistics_META)\nprint(summary_statistics_TSLA)\n\n            open        high         low       close  daily_return\nmean  180.497573  182.016635  179.110013  180.585403      0.004905\nstd     7.666419    7.603430    7.683683    7.581467      1.201368\nmin   166.910000  168.960000  165.670000  166.890000     -2.258081\n25%   173.995000  175.690000  173.162500  174.420000     -0.775974\n50%   178.395000  180.120000  177.217500  178.665000     -0.126516\n75%   186.755000  188.135000  185.065000  187.162500      0.710672\nmax   196.235000  198.230000  195.280000  196.450000      5.044233\n            open        high         low       close  daily_return\nmean  329.248129  332.421206  326.081446  329.164395     -0.082502\nstd    10.424783   10.871145    9.758547   10.254954      1.454917\nmin   308.000000  309.905000  306.090000  307.000000     -3.827645\n25%   321.450000  325.357475  319.920875  321.875000     -0.933508\n50%   331.055000  333.430000  327.120000  329.815000     -0.166908\n75%   335.222500  338.450000  332.885000  335.925000      0.741900\nmax   361.750000  366.780000  352.435000  359.490000      3.910945\n            open        high         low       close  daily_return\nmean  127.470927  129.022405  126.258075  127.643468     -0.113071\nstd     7.345721    7.142713    7.210315    7.182428      1.852893\nmin   105.180000  107.960000  105.160000  107.350000     -5.461437\n25%   122.362500  123.976250  121.195000  122.325000     -1.082018\n50%   128.335000  129.317500  126.890000  128.240000     -0.100464\n75%   133.357500  134.687500  131.940000  133.847500      0.848728\nmax   141.050000  141.220000  138.580000  140.550000     10.508717\n            open        high         low       close  daily_return\nmean  128.983661  130.550713  127.459491  129.020242     -0.188504\nstd     7.986347    7.903721    7.828463    7.879146      2.060386\nmin   105.040000  106.095000  104.700100  105.830000     -7.637745\n25%   125.557500  127.363750  124.130000  125.975000     -1.540472\n50%   129.580000  131.252500  128.204700  129.255000     -0.122718\n75%   133.995000  135.627500  132.822500  134.262500      0.913674\nmax   145.080000  145.860000  142.950000  144.850000      5.906582\n            open        high         low       close  daily_return\nmean  291.056734  295.337216  287.171990  291.285524     -0.218422\nstd    23.982296   23.841979   22.977277   23.244562      1.870259\nmin   231.415000  235.620000  230.270000  233.080000     -4.230675\n25%   280.902500  285.582500  277.255000  281.782500     -1.445339\n50%   297.830000  301.090000  293.350000  297.815000     -0.173350\n75%   307.420000  312.084750  301.685000  307.005000      0.877795\nmax   328.000000  330.540000  322.950000  327.820000      4.459209\n            open        high         low       close  daily_return\nmean  240.241921  245.035196  235.691160  240.501855     -0.098334\nstd    31.794378   31.973045   31.228469   31.588256      3.112003\nmin   165.650000  169.518400  164.350000  166.350000     -9.167337\n25%   220.613400  225.567500  216.215000  219.887500     -1.756820\n50%   249.850000  255.394950  244.535000  250.670000     -0.102090\n75%   261.102500  267.400000  257.319325  261.312500      1.699834\nmax   296.040000  299.290000  289.520100  293.340000     10.787372\n\n\n\n\n\n\n#Line plot for closing price for each company\nAAPL['timestamp'] = pd.to_datetime(AAPL['timestamp'])\nAAPL.set_index('timestamp', inplace=True)\nMSFT['timestamp'] = pd.to_datetime(MSFT['timestamp'])\nMSFT.set_index('timestamp', inplace=True)\nGOOGL['timestamp'] = pd.to_datetime(GOOGL['timestamp'])\nGOOGL.set_index('timestamp', inplace=True)\nAMZN['timestamp'] = pd.to_datetime(AMZN['timestamp'])\nAMZN.set_index('timestamp', inplace=True)\nMETA['timestamp'] = pd.to_datetime(META['timestamp'])\nMETA.set_index('timestamp', inplace=True)\nTSLA['timestamp'] = pd.to_datetime(TSLA['timestamp'])\nTSLA.set_index('timestamp', inplace=True)\n\nclosing_prices = pd.DataFrame({\n    'AAPL': AAPL['close'],\n    'MSFT': MSFT['close'],\n    'GOOGL': GOOGL['close'],\n    'AMZN': AMZN['close'],\n    'META': META['close'],\n    'TSLA': TSLA['close']\n})\nplt.figure(figsize=(15, 8))\nfor ticker in closing_prices.columns:\n    plt.plot(closing_prices.index, closing_prices[ticker], label=ticker)\n\nplt.title('Closing Prices Over Time')\nplt.xlabel('Date')\nplt.ylabel('Price (USD)')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Distribution for daily return\nimport seaborn as sns\nfor column in AAPL.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(AAPL[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in MSFT.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(MSFT[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in GOOGL.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(GOOGL[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in AMZN.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(AMZN[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in META.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(META[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\nfor column in TSLA.filter(regex='daily_return$').columns:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(TSLA[column].dropna(), bins=50, kde=True)\n    plt.title(f'Distribution of Daily Returns for {column}')\n    plt.xlabel('Daily Return')\n    plt.ylabel('Frequency')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndaily_returns = closing_prices.pct_change().dropna()\n\nsns.boxplot(data=daily_returns)\n\nplt.title('Box Plot of Daily Returns for Different Stocks')\nplt.ylabel('Daily Return')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\ncorrelation_matrix = daily_returns.corr()\n\nsns.heatmap(correlation_matrix, annot=True, cmap='Blues', linewidths=.5)\nplt.title('Correlation Matrix for Daily Returns of Stocks')\nplt.show()\n\n\n\n\n\n\n\nCan intraday price ranges be used to predict next-day stock volatility?\nHypothesis: There is a positive correlation between the intraday high-low spread and the volatility of the stock on the following day."
  }
]